{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e18657",
   "metadata": {},
   "source": [
    "This is the code to analyze the animes provided in the Top Anime website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe35f3",
   "metadata": {},
   "source": [
    "# Importing useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8596e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from math import ceil\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0aabb1",
   "metadata": {},
   "source": [
    "Here we send a request to get the content of a specific webpage. Here we wanted the content of the main page of the TopAnime webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da484abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopAnime = requests.get('https://myanimelist.net/topanime.php')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7d26d",
   "metadata": {},
   "source": [
    "Now we check if we have successfully received the content of the desired webpage or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850d0ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopAnime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34642c2b",
   "metadata": {},
   "source": [
    "As we have the code '200' as the status of the reponse, so it means that we have successfully received the desired page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec946e",
   "metadata": {},
   "source": [
    "Then we take a brief look at the content of this page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be13e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TopAnime.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7d0e9",
   "metadata": {},
   "source": [
    "As we can see in the output, we have the HTML code of the webpage. Now we should parse this HTML code to extract the URLs associated to each anime. To do so, we will be using **BeautifulSoap** library which is designed to parse HTML codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a64aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopAnimeSoup = BeautifulSoup(TopAnime.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebf4b9",
   "metadata": {},
   "source": [
    "Now we look at the produced HTML code of the webpage in a nicely format using BeautifulSoup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dff94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(TopAnimeSoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc4477",
   "metadata": {},
   "source": [
    "# 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cedf01",
   "metadata": {},
   "source": [
    "### Steps to extract the url for just one anime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824002b",
   "metadata": {},
   "source": [
    "Here we will go through the required steps to extract the url and the name of just the first anime in the list. We can get the information of the rest of these animes by just iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a315457",
   "metadata": {},
   "source": [
    "After chekcing the HTML code, we saw that the information related to each anime is stored in a table which its class is \"top-ranking-table\" and the animes' information are stored in trs of this table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c5ff4",
   "metadata": {},
   "source": [
    "Let's take a look at how many tables we have in the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2090573a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(TopAnimeSoup.find_all('table')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f69b3",
   "metadata": {},
   "source": [
    "As we have only one table in the webpage, so every tr that we have in the webpage belongs to this table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e3e65",
   "metadata": {},
   "source": [
    "Here we will take a look at how many tr tags we have in the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f39444ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(TopAnimeSoup.find_all('tr')))\n",
    "#len(list(TopAnimeSoup.find_all('tr')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb3233",
   "metadata": {},
   "source": [
    "We know that in each page, we have the information related to 50 animes. But why we have 51 tr tags here? <br/>\n",
    "Because the first row corresponds to the name of table's column and the rest store the inramtion related to each anime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3adde5",
   "metadata": {},
   "source": [
    "So in order to get the rows which contain the information of each anime, we should go through the tr tags that we have in the webpage except the first one which contains the information of the columns' name of of the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c46c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rows = list(TopAnimeSoup.find_all('tr'))[1:]\n",
    "len(Rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083511c1",
   "metadata": {},
   "source": [
    "As we can see above, we have all the rows correspond which contains the information related to each anime. The total number of animes we have in each page are 50. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726f486",
   "metadata": {},
   "source": [
    "Now we should get the name and the url correspond to each anime. We found out that this information can be found in 'a' tag of each row which its class name is \"hoverinfo_trigger fl-l ml12 mr8\" and is included in the second 'td' tag of each 'tr'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a4fe1",
   "metadata": {},
   "source": [
    "The information in this tag for the first anime can be found below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84976a7",
   "metadata": {},
   "source": [
    "We get all the 'td' tags of the second 'tr' the first 'tr' tag contains the columns' name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efb5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tds = Rows[0].find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc738a",
   "metadata": {},
   "source": [
    "Then we will go to the second 'td' tag's information. The first one contains just the ranking number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b986fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SecondTD = Tds[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee3663",
   "metadata": {},
   "source": [
    "Then inside this 'td' tag we look for the 'a' tag which its class is \"hoverinfo_trigger fl-l ml12 mr8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0041b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"hoverinfo_trigger fl-l ml12 mr8\" href=\"https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\" id=\"#area5114\" rel=\"#info5114\">\n",
       "<img alt=\"Anime: Fullmetal Alchemist: Brotherhood\" border=\"0\" class=\"lazyload\" data-src=\"https://cdn.myanimelist.net/r/50x70/images/anime/1223/96541.jpg?s=faffcb677a5eacd17bf761edd78bfb3f\" data-srcset=\"https://cdn.myanimelist.net/r/50x70/images/anime/1223/96541.jpg?s=faffcb677a5eacd17bf761edd78bfb3f 1x, https://cdn.myanimelist.net/r/100x140/images/anime/1223/96541.jpg?s=0c3b98cf4905422c00981025cd20d271 2x\" height=\"70\" width=\"50\">\n",
       "</img></a>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TagA = SecondTD.find('class' == \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "TagA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f92c35",
   "metadata": {},
   "source": [
    "The url of an anime is the value of 'href' property of this tag. Let's take a look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29a66360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = TagA['href']\n",
    "URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83e30e",
   "metadata": {},
   "source": [
    "Here to extract the name of the anime we have two options. 1. To split the 'href' and get the last value of it. 2. Get the value of 'alt' property of the 'img' tag in the 'a' tag. Here we will go for the second approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2fd39c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fullmetal Alchemist: Brotherhood'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image = TagA.find('img')\n",
    "AnimeName = Image['alt'].replace('Anime: ', '')\n",
    "AnimeName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ec208",
   "metadata": {},
   "source": [
    "### Going through of all animes in the first page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9662ece",
   "metadata": {},
   "source": [
    "Now here we want to get the name and url of all the animes in this specific page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b49d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyDict = defaultdict(str)\n",
    "for Row in Rows: \n",
    "    TDs = Row.find_all('td')\n",
    "    TagA = TDs[1].find('class' == \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "    AnimeName, URL = TagA.find('img')['alt'].replace('Anime: ', ''), TagA['href']\n",
    "    MyDict[AnimeName] = URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f686166",
   "metadata": {},
   "source": [
    "Now we will check the information for five animes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "164a1ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of anime: Fullmetal Alchemist: Brotherhood\n",
      "URL: https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\n",
      "\n",
      "Name of anime: Gintama°\n",
      "URL: https://myanimelist.net/anime/28977/Gintama°\n",
      "\n",
      "Name of anime: Shingeki no Kyojin Season 3 Part 2\n",
      "URL: https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2\n",
      "\n",
      "Name of anime: Steins;Gate\n",
      "URL: https://myanimelist.net/anime/9253/Steins_Gate\n",
      "\n",
      "Name of anime: Fruits Basket: The Final\n",
      "URL: https://myanimelist.net/anime/42938/Fruits_Basket__The_Final\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for anime in list(MyDict.keys())[:5]:\n",
    "    print('Name of anime: ' + anime+'\\nURL: ' + MyDict[anime], end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e461c8",
   "metadata": {},
   "source": [
    "Here we want to check if we have all the 50 animes' URL in the dictionay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef881261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of animes  50\n"
     ]
    }
   ],
   "source": [
    "print('Number of animes ', len(MyDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9c237",
   "metadata": {},
   "source": [
    "# 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479a68f",
   "metadata": {},
   "source": [
    "In order to have more readable implementation we will write a function which receives the URL of the webpage that we want to scrap and writes the urls to the URLs.txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "596c02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAnimeInfo(webpage):\n",
    "    File = open('URLs.txt', mode = 'a')\n",
    "    Request = requests.get(webpage)\n",
    "    TopAnimeSoup = BeautifulSoup(Request.content, 'html.parser')\n",
    "    Rows = TopAnimeSoup.find_all('tr')\n",
    "    for Row in Rows[1:]: \n",
    "        TDs = Row.find_all('td')\n",
    "        TagA = TDs[1].find('class' == \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "        URL = TagA['href']\n",
    "        File.write(URL+'\\n')\n",
    "    File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8a2bc",
   "metadata": {},
   "source": [
    "Now at each time we should pass the function the URL of the webpage that we want to scrap. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f11a76",
   "metadata": {},
   "source": [
    "After checking the URL of the next pages, we understood that there is a pattern in URL of the pages. <br/>\n",
    "For example the 2nd webpage's URL is 'https://myanimelist.net/topanime.php?limit=50' and we can see the only difference between this URL and the main page URL (which is 'https://myanimelist.net/topanime.php') is that there is '?limit=50' string at the end. <br/><br/>\n",
    "* So we can use this pattern the find the next pages URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "93fc9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "MainPageURL = 'https://myanimelist.net/topanime.php'\n",
    "for i in range(400):\n",
    "    if i == 0:\n",
    "        GetAnimeInfo(MainPageURL)\n",
    "    else:\n",
    "        GetAnimeInfo(MainPageURL+'?limit='+str(50*i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c658f",
   "metadata": {},
   "source": [
    "Now we want to download the HTML file for each anime. <br/> \n",
    "In order to do this we are going to write a function that passes a number of URLs which their HTML file should be downloaded and stored in a folder.<br/>\n",
    "These URLS are given in a list of list which correspond to a number of webpages in the website and a number of anime's URL which are in a specific webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2dfe0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is designed to download the URLs which are contained in some specific webpages. \n",
    "\n",
    "def GetHTML(URLS, GroupIndex, NumPageGroup, Start):\n",
    "    for index, ListURLs in enumerate(URLS):\n",
    "        if index < Start:\n",
    "            continue\n",
    "        FolderName = 'page' + str(GroupIndex*NumPageGroup + index +1)\n",
    "        Path = os.path.join(os.getcwd(),'HTMLS', FolderName)\n",
    "        # \"/\".join([os.getcwd(),'HTMLS', FolderName])\n",
    "        # /home/mehrdad/ADM-HW3/HTMLS/page1\n",
    "        os.mkdir(Path)\n",
    "        for i, url in enumerate(ListURLs):\n",
    "            AnimePage = requests.get(url)\n",
    "            AnimePageSoup = BeautifulSoup(AnimePage.content, 'html.parser')\n",
    "            #/home/mehrdad/ADM-HW3/HTMLS/page1/anime_0.html\n",
    "            file = open(Path + '/' + 'anime_' + str(i) + '.html',\"w\")\n",
    "            file.write(str(AnimePageSoup))\n",
    "            file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a53d6",
   "metadata": {},
   "source": [
    "Here we want to mention what are the arguments of this function exactly:\n",
    "* URLS: It's a matrix. Each row corresponds to a page and the columns are the URls which are contained in a specific page. \n",
    "* GroupIndex: We specify on which core of processor this process is being done. \n",
    "* NumPageGroup: Corresponds to the number of pages that is being given to the function. \n",
    "* Start: From which page start downloading its animes' URLs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76c90d",
   "metadata": {},
   "source": [
    "Here in order to speed up the process of downloading HTML files, we want to parallelize this process among the number of processors that we have in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc688f2",
   "metadata": {},
   "source": [
    "So first we check how many processors we have in the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79c621f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of processors: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of processors:\", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590ada1",
   "metadata": {},
   "source": [
    "So in this case we should distribute our URLS over these processors. So here we get our URLs from the file that we stored all the URLs there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ead245d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllURLs = []\n",
    "with open('URLs.txt') as f:\n",
    "    for row in f:\n",
    "        AllURLs.append(row.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4a437",
   "metadata": {},
   "source": [
    "Now we want to check how many URLs we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d2a928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of URLs is: 19128\n"
     ]
    }
   ],
   "source": [
    "print('The total number of URLs is:', len(AllURLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf4889",
   "metadata": {},
   "source": [
    "* **Note:** We know that we should store all the anime's webpage HTML in a single folder for the ones that have been appeared in the same wepbage, so we every webpage's anime should be entirely given to a singel processors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfaeeff",
   "metadata": {},
   "source": [
    "* **Note:** We know that we got the URLs in order, so each 50 URLs are the ones that were in a specific webpage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60848ee4",
   "metadata": {},
   "source": [
    "Now we group each 50 URls which correspond to the animes that are in the same webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3730c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EachPageURLs = [AllURLs[i*50:(i+1)*50] for i in range(len(AllURLs)//50+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c82bf6",
   "metadata": {},
   "source": [
    "Here we will take a look at how many pages that we have here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "372960e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of webpages in the website: 383\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of webpages in the website:\", len(EachPageURLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9827d77",
   "metadata": {},
   "source": [
    "So if this number of webpages is true, then the division 'number of all URLS'/50 shoule produce the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc235115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of division: 382.56\n"
     ]
    }
   ],
   "source": [
    "print('The result of division:', len(AllURLs)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0074706",
   "metadata": {},
   "source": [
    "As we know that the last webpage does not contain exactly 50 animes, we got this result, so up until now everything went great. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59759467",
   "metadata": {},
   "source": [
    "Now we check how many pages shold be given to each of the processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d143748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages to be given to each processor:  96\n"
     ]
    }
   ],
   "source": [
    "NumOfPage = ceil(ceil(len(AllURLs)/50)/4)\n",
    "print('Number of pages to be given to each processor: ', NumOfPage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad70465",
   "metadata": {},
   "source": [
    "So we will give 96 pages to each of the processor that we have in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8202b99",
   "metadata": {},
   "source": [
    "Now we will put the pages should be given to a specific processor into groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "700c5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupPage = [EachPageURLs[i*NumOfPage:(i+1)*NumOfPage] for i in range(mp.cpu_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b846a4",
   "metadata": {},
   "source": [
    "Let's check the number of pages in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "508216ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 96, 96, 95]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(len , GroupPage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad966b",
   "metadata": {},
   "source": [
    "It can be seen the first 3 processors will be given 96 pages and the last one 95 pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31568fe2",
   "metadata": {},
   "source": [
    "### Downloading HTMLs in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79014bf9",
   "metadata": {},
   "source": [
    "As we saw before, we have 4 processors in the system and we want to distribute the work among these 4 processors.<br/>\n",
    "We will give a subsets of pages to each of these processors to download their animes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9a861a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = [pool.apply_async(GetHTML, args = (GroupPage[i],i, NumOfPage, 5)) for i in range(mp.cpu_count())]\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51699a6e",
   "metadata": {},
   "source": [
    "# 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007d5e3",
   "metadata": {},
   "source": [
    "Now in this question we should produce a .tsv file for the desired information for each of the pages that we have just downloaded in the previous section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60100bf2",
   "metadata": {},
   "source": [
    "In order to do this, first we check how we can get the information for just one anime and then we will expand the idea to other anime that we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791438b",
   "metadata": {},
   "source": [
    "The information that we should extract for each anime are: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58e960",
   "metadata": {},
   "source": [
    "1- **Anime Name** (to save as animeTitle): String <br/>\n",
    "2- **Anime Type** (to save as animeType): String<br/>\n",
    "3- **Number of episode** (to save as animeNumEpisode): Integer<br/>\n",
    "4- **Release and End Dates of anime** (to save as releaseDate and endDate): Convert both release and end date into datetime format.<br/>\n",
    "5- **Number of members** (to save as animeNumMembers): Integer<br/>\n",
    "6- **Score** (to save as animeScore): Float<br/>\n",
    "7- **Users** (to save as animeUsers): Integer<br/>\n",
    "8- **Rank** (to save as animeRank): Integer<br/>\n",
    "9- **Popularity** (to save as animePopularity): Integer<br/>\n",
    "10- **Synopsis** (to save as animeDescription): String<br/>\n",
    "11- **Related Anime** (to save as animeRelated): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.<br/>\n",
    "12- **Characters** (to save as animeCharacters): List of strings.<br/>\n",
    "13- **Voices** (to save as animeVoices): List of strings<br/>\n",
    "14- **Staff** (to save as animeStaff): Include the staff name and their responsibility/task in a list of lists.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5adca0",
   "metadata": {},
   "source": [
    "The for getting each of the information that we need, we write a function to have a better understanding of how we should extract each of these information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49d395",
   "metadata": {},
   "source": [
    "### 1.3.1 Function AnimeName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740b0f2",
   "metadata": {},
   "source": [
    "We know that the name of the anime can be extracted from the title of the webpage. So we send the html code of the webpage to this function, and we will return the name of the anime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb7925fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAnimeName(webpage):\n",
    "    return webpage.title.getText().strip().replace(' - MyAnimeList.net', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f197588",
   "metadata": {},
   "source": [
    "### 1.3.2 Function GetAnimeType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f6c2c",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7eb45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAnimeType(Tag):\n",
    "    Temp = Tag.getText().split()\n",
    "    return Temp[-1] if len(Temp) >1 else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7cbdc",
   "metadata": {},
   "source": [
    "### 1.3.3 Function Episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672faa7",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9e85e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNumOfEpisode(Tag):\n",
    "    Temp = Tag.getText().strip().split()[-1]\n",
    "    return int(Temp) if Temp.isdigit() else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba1c47",
   "metadata": {},
   "source": [
    "### 1.3.4 Function DateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a0ebc",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime. <br/><br/>\n",
    "* Some of the animes may have just the release date. So we should be careful about this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "853481f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDates(Tag):\n",
    "    Release, End = '', ''\n",
    "    Temp = Tag.getText().strip().replace('Aired:\\n  ', '').split('to')\n",
    "    Release = Temp[0] if len(Temp) else ''\n",
    "    End = Temp[1] if len(Temp) == 2 else ''\n",
    "    return Release, End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc18dd0",
   "metadata": {},
   "source": [
    "### 1.3.5 Function Members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21b155",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa01c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMembers(Tag):\n",
    "    return int(Tag.getText().replace('\\n', '').split()[1].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fd01e",
   "metadata": {},
   "source": [
    "### 1.3.6 Function ScoreAndUsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b78920",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0241caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetScoreAndUsers(Tag):\n",
    "    Rank = re.findall('[0-9|,]+ users', str(Tag))\n",
    "    RankValue = int(Rank[0].split()[0].replace(',', '')) if len(Rank) else ''\n",
    "    Score = re.findall('Score:[0-9|.]+', Tag.getText().replace(\"\\n\", ''))\n",
    "    ScoreValue = float(Score[0].split(\":\")[1]) if len(Score) else ''\n",
    "    return ScoreValue, RankValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e7525",
   "metadata": {},
   "source": [
    "### 1.3.7 Function Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48170a3",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90eaa92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRank(Tag):\n",
    "    Temp = re.findall('#[0-9|,]+', str(Tag))\n",
    "    return int(Temp[0][1:]) if len(Temp) else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e43b5c",
   "metadata": {},
   "source": [
    "### 1.3.8 Function Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757290a",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8f196db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPopularity(Tag):\n",
    "    Temp = re.findall('#[0-9|,]+', str(Tag))\n",
    "    return int(Temp[0][1:]) if len(Temp) else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfec584",
   "metadata": {},
   "source": [
    "### 1.3.9 Function Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078f597",
   "metadata": {},
   "source": [
    "In order to get the description of the anime, there is a specific tag which can be easily identified by its 'itemprop' property. So we will give the whole html to this function and get back the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd782ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSynopsis(webpage):\n",
    "    Temp = webpage.find('p', {'itemprop': \"description\"})\n",
    "    if not Temp:\n",
    "        return ''\n",
    "    Temp = Temp.getText().replace('\\n', '')\n",
    "    return Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a54bf",
   "metadata": {},
   "source": [
    "### 1.3.10 Function Related Anime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d8537",
   "metadata": {},
   "source": [
    "To get the related animes, there is a table which its 'class' is equal to 'anime_detail_related_anime'. So to get the table we should give the function the whole html file to this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30bf71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRelatedAnime(webpage):\n",
    "    Related = webpage.find('table', {'class': \"anime_detail_related_anime\"})\n",
    "    if not Related:\n",
    "        return \"\"\n",
    "    Related = Related.find_all('a')\n",
    "    UniqueRelated = set(i.getText() for i in Related)\n",
    "    return list(UniqueRelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3adbe",
   "metadata": {},
   "source": [
    "### 1.3.11 Function Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c635727",
   "metadata": {},
   "source": [
    "We can find this information in tag 'a' that are included in a div which is its class is equal to 'detail-characters-list clearfix'. When we go for the first div that have this feature we will be received the table which is the characters and their original voices is there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95f10ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCharacters(webpage):\n",
    "    Characters = []\n",
    "    Tags = webpage.find_all('div', {'class': \"detail-characters-list clearfix\"})\n",
    "    for tag in Tags:\n",
    "        if str(tag).count('character') >1:\n",
    "            # We can get the name of the character in the 'href' value in 'a' property\n",
    "            AllTagA = tag.find_all('a')\n",
    "            # Filter the hrefs to characters\n",
    "            CharactersHrefs = list(set([i['href'] for i in AllTagA if 'character' in i['href']]))\n",
    "            # Filter the names of the characters\n",
    "            Characters = [i.split('/')[-1].replace('_', ' ') for i in CharactersHrefs]\n",
    "    return Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871b822",
   "metadata": {},
   "source": [
    "### 1.3.12 Function Voices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e19f7",
   "metadata": {},
   "source": [
    "With the same approach as previous, now we just extract the voices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e664b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVoices(webpage):\n",
    "    Voices = []\n",
    "    Tags = webpage.find_all('div', {'class': \"detail-characters-list clearfix\"})\n",
    "    for tag in Tags:\n",
    "        if str(tag).count('character') > 1:\n",
    "            # We can get the name of the person in charge for the voice in the 'href' value in 'a' property\n",
    "            AllTagA = tag.find_all('a')\n",
    "            # Filter the hrefs for voices\n",
    "            VoicesHrefs = list(set([i['href'] for i in AllTagA if 'people' in i['href']]))\n",
    "            # Filter the names of the people\n",
    "            Voices = [i.split('/')[-1].replace('_', ' ') for i in VoicesHrefs]\n",
    "    return Voices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bd8ed",
   "metadata": {},
   "source": [
    "### 1.3.13 Function Staff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90395d78",
   "metadata": {},
   "source": [
    "Here in this function we will look for the other div tha its 'class' is equal to 'detail-characters-list clearfix'. Then the name of the staff can be extracted from 'img' tags and also their duty can be found in the 'small' tags which are in the same 'row' as their image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a628582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStaff(webpage):\n",
    "    StaffDuty = []\n",
    "    Tags = webpage.find_all('div', {'class': \"detail-characters-list clearfix\"})\n",
    "    for tag in Tags:\n",
    "        if str(tag).count('character') == 1:\n",
    "            Staff =  tag.find_all('tr')\n",
    "            for i in Staff:\n",
    "                NewStaff = [i.find('a')['href'].split('/')[-1].replace('_', ' ')] # Extracting the name of the staff\n",
    "                Duties = list(i.find('small').getText().split(',')) # Getting the duties of the staff\n",
    "                NewStaff += [i.strip() for i in Duties]\n",
    "                StaffDuty.append(NewStaff)\n",
    "    return StaffDuty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbd662",
   "metadata": {},
   "source": [
    "### 1.3.14 Function Write to TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c4bfd",
   "metadata": {},
   "source": [
    "With the help of this function, we can write the information that we extracted from a page to its corresponded .tsv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "075d839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteToTSV(AnimeInfo, Path):\n",
    "    File = open(Path, mode = 'w')\n",
    "    # Writing the header. \n",
    "    File.write(\"\\t\".join(Keys))\n",
    "    File.write('\\n')\n",
    "    for i in AnimeInfo:\n",
    "        STR = AnimeInfo[i].__str__()\n",
    "        if STR == '[]':\n",
    "            STR = ''\n",
    "        File.write(STR + ('\\t' if i!='animeStaff' else \"\" ))\n",
    "    File.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2bc03",
   "metadata": {},
   "source": [
    "### 1.3.15 Function Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a5252",
   "metadata": {},
   "source": [
    "With the help of this function we want to extract the desired information from all the .html files that we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34553752",
   "metadata": {},
   "source": [
    "Here we used some key words so we can easily get the needed information which are in some tags that share the same class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df00d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Info = ['Type:', 'Episodes:', 'Aired:', 'Members:','Score:','Ranked:', 'Popularity:']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ab353",
   "metadata": {},
   "source": [
    "Here we define the default datastructure which is a dictionary to store the inforamtion of each page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e03cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keys = ['animeTitle', 'animeType', 'animeNumEpisode', 'releaseDate', 'endDate', 'animeNumMembers'\n",
    "        , 'animeScore', 'animeUsers', 'animeRank', 'animePopularity', 'animeDescription', 'animeRelated',\n",
    "        'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "MyAnimeInfo = {i:\"\" for i in Keys} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745991de",
   "metadata": {},
   "source": [
    "Here we will go through each HTML file that we have just downloaded. Then we extract the infomration that we need from the HTML file and then store them in a .tsv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46254389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionExtract(ListNumPage, ProcesserNum):\n",
    "    for i in ListNumPage:\n",
    "        for j in range((50 if i!= 383 else 28)):\n",
    "            \n",
    "            exec('NewAnime'+str(ProcesserNum)+'= MyAnimeInfo.copy()')\n",
    "            \n",
    "            Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "            File = open(Path + str(i) + '/anime_' + str(j) +'.html', mode = 'r')\n",
    "            \n",
    "            Webpage = BeautifulSoup(File.read(), 'html.parser')\n",
    "            \n",
    "            for div in Webpage.find_all('div', {'class':\"spaceit_pad\"}):\n",
    "                if Info[0] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeType'] = GetAnimeType(div)\")\n",
    "                elif Info[1] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeNumEpisode'] = GetNumOfEpisode(div)\")\n",
    "                elif Info[2] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['releaseDate'] = GetDates(div)[0]\")\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['endDate'] = GetDates(div)[1]\")\n",
    "                elif Info[3] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeNumMembers'] = GetMembers(div)\")\n",
    "                elif Info[4] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeScore'] = GetScoreAndUsers(div)[0]\")\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeUsers'] = GetScoreAndUsers(div)[1]\")\n",
    "                elif Info[5] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeRank'] = GetRank(div)\")\n",
    "                elif Info[6] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animePopularity'] = GetPopularity(div)\")\n",
    "                \n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeTitle'] = GetAnimeName(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeDescription'] = GetSynopsis(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeRelated'] = GetRelatedAnime(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeCharacters'] = GetCharacters(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeVoices'] = GetVoices(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeStaff'] = GetStaff(Webpage)\")\n",
    "            \n",
    "            TSVPath = Path + str(i) + '/anime_' + str(j) +'.tsv'\n",
    "            exec('WriteToTSV(NewAnime'+ str(ProcesserNum)+',TSVPath)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53e121",
   "metadata": {},
   "source": [
    "### Creating the .tsv files in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe78910",
   "metadata": {},
   "source": [
    "* **In order to speed up the process, we will distribute the work among the available CPUs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f02ae8",
   "metadata": {},
   "source": [
    "We should give a subset of pages to each CPU to make its animes' .tsv file. <br/>\n",
    "Here we will group the page numbers that should be given to each processor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa13be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "RangePage = list(range(1, len(EachPageURLs) + 1))\n",
    "PageNums = [RangePage[i * NumOfPage:(i+1) * NumOfPage] for i in range(mp.cpu_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03391175",
   "metadata": {},
   "source": [
    "Call the function for each CPU give a susbset of .html files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c0d68296",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = [pool.apply_async(FunctionExtract, args = (PageNums[i],i)) for i in range(mp.cpu_count())]\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329d0e4",
   "metadata": {},
   "source": [
    "# 2. Search Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ff9c1",
   "metadata": {},
   "source": [
    "Here we are asked to build a search engine which given a query, will give back the documents that are similar to the given query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da72a3",
   "metadata": {},
   "source": [
    "### 2.0 Pre-processing the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93de6a6",
   "metadata": {},
   "source": [
    "Here we will pre-process all the information of an anime and store it to another .tsv file which will be name SynopsisPrepAnime_(i).csv which i corresponds to the index number of the page in its page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8bdaa",
   "metadata": {},
   "source": [
    "To do pre-processing we have 5 stages and for each stage we will write a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca972cc",
   "metadata": {},
   "source": [
    "### 2.0.1 Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36c4c4",
   "metadata": {},
   "source": [
    "Given a string, we will return the words that are in the given string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd722bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(Sentence):\n",
    "    return nltk.word_tokenize(Sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe887d36",
   "metadata": {},
   "source": [
    "### 2.0.2 Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368181a",
   "metadata": {},
   "source": [
    "Given a list of strings, we will return the same strings but in lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a20bdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lowercasing(Words):\n",
    "    return [w.lower() for w in Words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f0b34",
   "metadata": {},
   "source": [
    "### 2.0.3 StopWordsRemoval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a8a51",
   "metadata": {},
   "source": [
    "Here we will define a list which contains all the stopwords in English language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01cd6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74986fc2",
   "metadata": {},
   "source": [
    "Given a list of strings, we will remove the stopwords from that strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0fa662c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWordsRemoval(Words):\n",
    "    return [w for w in Words if w not in StopWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d02f8",
   "metadata": {},
   "source": [
    "### 2.0.4 PunctuationsRemoval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e449f",
   "metadata": {},
   "source": [
    "In this function given a list of strings, we will remove the punctuations from those strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0be23c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PunctuationsRemoval(Words):\n",
    "    return [w for w in Words if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24313fc3",
   "metadata": {},
   "source": [
    "### 2.0.5 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccbab2",
   "metadata": {},
   "source": [
    "In this function, given a list of strings, we try to return back the stem of each string that we have in the list. <br/>\n",
    "Here we will use 'PorterStemmer' algorithm to do stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8e437df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(Words):\n",
    "    return [PorterStemmer().stem(w) for w in Words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68d729",
   "metadata": {},
   "source": [
    "* Here we are asked to work with the 'Synopsis' of each anime, so we decided pre-process only the synopsis of each anime and not all the information that we extracted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14233c22",
   "metadata": {},
   "source": [
    "* **Note:** For your information, we want to do the pre-process we will distribute the work among the available number of CPUs in the system. Each CPU will be given a subset of pages to do the pre-process for the contained anime information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac440f57",
   "metadata": {},
   "source": [
    "### 2.0.6 Main Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f0a5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MainFunction(Pages):\n",
    "    for page in Pages:\n",
    "        for anime in range((50 if page != 383 else 28)):\n",
    "            \n",
    "            Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "            File = open(Path + str(page) + '/anime_' + str(anime) +'.tsv', mode = 'r')\n",
    "            # For each anime we want to extract the synopsis \n",
    "            Data = File.read().split('\\n')[1].split('\\t')[10]\n",
    "            File.close()\n",
    "            \n",
    "            Tokens = Tokenization(Data)\n",
    "            Lowercase = Lowercasing(Tokens)\n",
    "            WithoutStop = StopWordsRemoval(Lowercase)\n",
    "            WithoutPuncs = PunctuationsRemoval(WithoutStop)\n",
    "            Stems = Stemming(WithoutPuncs)\n",
    "            \n",
    "            PreProcessed = open(Path + str(page) + '/anime_' + str(anime) +'_synopsisPrep.csv', mode = 'w')\n",
    "            PreProcessed.write(\",\".join(Stems))\n",
    "            PreProcessed.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0232e",
   "metadata": {},
   "source": [
    "Group the pages to be given to the CPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "43338b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RangePage = list(range(1, len(EachPageURLs) + 1))\n",
    "PageNums = [RangePage[i * NumOfPage:(i+1) * NumOfPage] for i in range(mp.cpu_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02733231",
   "metadata": {},
   "source": [
    "Pre-processing all the synopsis of animes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6fc0f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = [pool.apply_async(MainFunction, args = (PageNums[i],)) for i in range(mp.cpu_count())]\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = 1, 0\n",
    "Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "File = open(Path + str(i) + '/anime_' + str(j) +'.html', mode = 'r')\n",
    "Webpage = BeautifulSoup(File.read(), 'html.parser')\n",
    "GetStaff(Webpage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
