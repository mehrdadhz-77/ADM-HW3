{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e18657",
   "metadata": {},
   "source": [
    "# 3rd Homework of ADM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308dc4e",
   "metadata": {},
   "source": [
    "### Group members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d41aa",
   "metadata": {},
   "source": [
    "#### 1. Mehrdad Hassanzadeh, 1961575, hassanzadeh.1961575@studenti.uniroma1.it\n",
    "#### 2. Vahid Ghanbarizadeh, 2002527, ghanbarizdehv@gmail.com\n",
    "#### 3. Andrea Giordano , 1871786, giordano.1871786@studenti.uniroma1.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe35f3",
   "metadata": {},
   "source": [
    "# Importing useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8596e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from math import ceil\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from copy import deepcopy\n",
    "from heapq import heappop, heappush  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe9188",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7097bae",
   "metadata": {},
   "source": [
    "We are asked to collect some information about the anime in a spcific website. Here is the link to that site. 'https://myanimelist.net/topanime.php'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0aabb1",
   "metadata": {},
   "source": [
    "Here we send a request to get the content of a specific webpage. We wanted the content of the main page of the TopAnime website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da484abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopAnime = requests.get('https://myanimelist.net/topanime.php')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7d26d",
   "metadata": {},
   "source": [
    "Now we check if we have successfully received the content of the desired webpage or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850d0ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopAnime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34642c2b",
   "metadata": {},
   "source": [
    "As we have the code '200' as the status of the reponse, so it means that we have successfully received the desired page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec946e",
   "metadata": {},
   "source": [
    "Then we take a brief look at the content of this page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be13e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TopAnime.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7d0e9",
   "metadata": {},
   "source": [
    "As we can see in the output, we have the HTML code of the webpage. Now we should parse this HTML code in order to get the data we want. To do so, we will be using **BeautifulSoap** library which is designed to parse HTML codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a64aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopAnimeSoup = BeautifulSoup(TopAnime.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebf4b9",
   "metadata": {},
   "source": [
    "Now we look at the produced HTML code of the webpage in nice and more readable format using BeautifulSoup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dff94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(TopAnimeSoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc4477",
   "metadata": {},
   "source": [
    "# 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cedf01",
   "metadata": {},
   "source": [
    "### Steps to extract the url for just one anime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824002b",
   "metadata": {},
   "source": [
    "Here we will go through the required steps to extract the url and the name of just the first anime in the list. We can then expand this idea to all the anime in the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763d25b",
   "metadata": {},
   "source": [
    "### 1.1.1 Getting all the rows (anime) in a page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a315457",
   "metadata": {},
   "source": [
    "After chekcing the HTML code, we saw that the information related to each anime is stored in a table which its class is \"top-ranking-table\" and the animes' information are stored in trs of this table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c5ff4",
   "metadata": {},
   "source": [
    "Let's take a look at how many tables we have in the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2090573a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(TopAnimeSoup.find_all('table')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f69b3",
   "metadata": {},
   "source": [
    "As we have only one table in the webpage, so every tr that we have in the webpage belongs to this table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e3e65",
   "metadata": {},
   "source": [
    "Here we will take a look at how many tr tags we have in the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f39444ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(TopAnimeSoup.find_all('tr')))\n",
    "#len(list(TopAnimeSoup.find_all('tr')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb3233",
   "metadata": {},
   "source": [
    "We know that in each page, we have the information related to 50 animes. But why we have 51 tr tags here? <br/>\n",
    "Because the first row corresponds to the name of table's column and the rest store the inramtion related to each anime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3adde5",
   "metadata": {},
   "source": [
    "So in order to get the rows which contain the information of each anime, we should go through the tr tags that we have in the webpage except the first one which contains the information of the columns' name of of the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c46c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rows = list(TopAnimeSoup.find_all('tr'))[1:]\n",
    "len(Rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083511c1",
   "metadata": {},
   "source": [
    "Here we have all the information of the anime in a specific page. As we know the number of anime in each page is 50. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5396d3e",
   "metadata": {},
   "source": [
    "### 1.1.2 Extracting the name and url of an anime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726f486",
   "metadata": {},
   "source": [
    "Now we should get the name and the url correspond to each anime. We found out that this information can be found in 'a' tag of each row which its class name is \"hoverinfo_trigger fl-l ml12 mr8\" and is included in the second 'td' tag of each 'tr'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84976a7",
   "metadata": {},
   "source": [
    "We get all the 'td' tags of the second 'tr' the first 'tr' tag contains the columns' name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efb5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tds = Rows[0].find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc738a",
   "metadata": {},
   "source": [
    "Then we will go to the second 'td' tag's information. The first one contains just the ranking number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b986fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SecondTD = Tds[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee3663",
   "metadata": {},
   "source": [
    "Then inside this 'td' tag we look for the 'a' tag which its class is \"hoverinfo_trigger fl-l ml12 mr8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0041b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"hoverinfo_trigger fl-l ml12 mr8\" href=\"https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\" id=\"#area5114\" rel=\"#info5114\">\n",
       "<img alt=\"Anime: Fullmetal Alchemist: Brotherhood\" border=\"0\" class=\"lazyload\" data-src=\"https://cdn.myanimelist.net/r/50x70/images/anime/1223/96541.jpg?s=faffcb677a5eacd17bf761edd78bfb3f\" data-srcset=\"https://cdn.myanimelist.net/r/50x70/images/anime/1223/96541.jpg?s=faffcb677a5eacd17bf761edd78bfb3f 1x, https://cdn.myanimelist.net/r/100x140/images/anime/1223/96541.jpg?s=0c3b98cf4905422c00981025cd20d271 2x\" height=\"70\" width=\"50\">\n",
       "</img></a>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TagA = SecondTD.find('class' == \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "TagA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f92c35",
   "metadata": {},
   "source": [
    "The url of an anime is the value of 'href' property of this tag. Let's take a look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29a66360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = TagA['href']\n",
    "URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83e30e",
   "metadata": {},
   "source": [
    "Here to extract the name of the anime we have two options. 1. To split the 'href' and get the last value of it. 2. Get the value of 'alt' property of the 'img' tag in the 'a' tag. Here we will go for the second approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2fd39c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fullmetal Alchemist: Brotherhood'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image = TagA.find('img')\n",
    "AnimeName = Image['alt'].replace('Anime: ', '')\n",
    "AnimeName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ec208",
   "metadata": {},
   "source": [
    "### 1.1.3 Getting the information for all anime in a page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9662ece",
   "metadata": {},
   "source": [
    "Now here we want to get the name and url of all the animes in this specific page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b49d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the url or each anime in the page \n",
    "MyDict = defaultdict(str)\n",
    "\n",
    "# Going through all the rows in the page\n",
    "for Row in Rows: \n",
    "    \n",
    "    # Take the second column of the row\n",
    "    TDs = Row.find_all('td')\n",
    "    \n",
    "    # Take the tag a which consists the anime name and the url of that anime\n",
    "    TagA = TDs[1].find('class' == \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "    \n",
    "    # Extract the anime name and url of the anime\n",
    "    AnimeName, URL = TagA.find('img')['alt'].replace('Anime: ', ''), TagA['href']\n",
    "    \n",
    "    # Store the url of the anime in the dictionary\n",
    "    MyDict[AnimeName] = URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f686166",
   "metadata": {},
   "source": [
    "Now we will check the information for five animes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "164a1ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of anime: Fullmetal Alchemist: Brotherhood\n",
      "URL: https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\n",
      "\n",
      "Name of anime: Gintama°\n",
      "URL: https://myanimelist.net/anime/28977/Gintama°\n",
      "\n",
      "Name of anime: Shingeki no Kyojin Season 3 Part 2\n",
      "URL: https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2\n",
      "\n",
      "Name of anime: Steins;Gate\n",
      "URL: https://myanimelist.net/anime/9253/Steins_Gate\n",
      "\n",
      "Name of anime: Fruits Basket: The Final\n",
      "URL: https://myanimelist.net/anime/42938/Fruits_Basket__The_Final\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for anime in list(MyDict.keys())[:5]:\n",
    "    print('Name of anime: ' + anime+'\\nURL: ' + MyDict[anime], end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e461c8",
   "metadata": {},
   "source": [
    "Here we want to check if we have all the 50 animes' URL in the dictionay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef881261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of animes  50\n"
     ]
    }
   ],
   "source": [
    "print('Number of animes ', len(MyDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9c237",
   "metadata": {},
   "source": [
    "# 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca68959",
   "metadata": {},
   "source": [
    "### 1.2.1 Function Crawl urls in a webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479a68f",
   "metadata": {},
   "source": [
    "In order to have more readable implementation we will write a function which receives the URL of the webpage that we want to scrap and writes the urls to the URLs.txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "596c02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAnimeInfo(webpage):\n",
    "    \n",
    "    # Opening the file that we want to write our information in \n",
    "    File = open('URLs.txt', mode = 'a')\n",
    "    \n",
    "    # Send a request to get the webpage\n",
    "    Request = requests.get(webpage)\n",
    "    \n",
    "    # Parse the HTML code of the webpage\n",
    "    TopAnimeSoup = BeautifulSoup(Request.content, 'html.parser')\n",
    "    \n",
    "    # Take all the tr tags in the webpage which contain all the anime information in the given webpage\n",
    "    Rows = TopAnimeSoup.find_all('tr')\n",
    "    \n",
    "    # Going throguh all the anime \n",
    "    for Row in Rows[1:]: \n",
    "        \n",
    "        # Get the td tags of each anime \n",
    "        TDs = Row.find_all('td')\n",
    "        \n",
    "        # Take the tag A which its class is equal to 'hoverinfo_trigger fl-l ml12 mr8'\n",
    "        TagA = TDs[1].find('class' == \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "        \n",
    "        # Extract the URL of the anime\n",
    "        URL = TagA['href']\n",
    "        \n",
    "        # Write the URL to the file\n",
    "        File.write(URL+'\\n')\n",
    "        \n",
    "    # Close the file we have just written it\n",
    "    File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8a2bc",
   "metadata": {},
   "source": [
    "Now at each time we should pass the function the URL of the webpage that we want to scrap. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f11a76",
   "metadata": {},
   "source": [
    "After checking the URL of the next pages, we understood that there is a pattern in URL of the pages. <br/>\n",
    "For example the 2nd webpage's URL is 'https://myanimelist.net/topanime.php?limit=50' and we can see the only difference between this URL and the main page URL (which is 'https://myanimelist.net/topanime.php') is that there is '?limit=50' string at the end. <br/><br/>\n",
    "* So we can use this pattern the find the next pages URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be03f661",
   "metadata": {},
   "source": [
    "### 1.2.2 Main Function to extract all the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "93fc9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the root webpage of the website\n",
    "MainPageURL = 'https://myanimelist.net/topanime.php'\n",
    "\n",
    "# We go through first 400 pages \n",
    "for i in range(400):\n",
    "    \n",
    "    # This is the address that we want for the first page\n",
    "    if i == 0:\n",
    "        GetAnimeInfo(MainPageURL)\n",
    "        \n",
    "    # For the next pages, we should use the address of the webpage and put the limit \n",
    "    # at the end to reach to the next pages\n",
    "    else:\n",
    "        GetAnimeInfo(MainPageURL+'?limit='+str(50*i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e609f",
   "metadata": {},
   "source": [
    "### 1.2.3 Function Get HTML code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c658f",
   "metadata": {},
   "source": [
    "Now we want to download the HTML file for each anime. <br/> \n",
    "In order to do this we are going to write a function that passes a number of URLs which their HTML file should be downloaded and stored in a folder.<br/>\n",
    "These URLS are given in a list of list which correspond to a number of webpages in the website and a number of anime's URL which are in a specific webpage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d63545",
   "metadata": {},
   "source": [
    "* **Note:** We will download the HTML codes in parallel by using all the processors in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4fbba0",
   "metadata": {},
   "source": [
    "As this function will be called from all the processors in the system, we should find a way so that all the processors can download their own given pages' HTML code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2dfe0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters of the function: \n",
    "\n",
    "# URLS: A list of a list which each element in the first list is the pages and each element in the list \n",
    "# of each page is the anime's URL in that page that we should get their HTML code. \n",
    "\n",
    "# GroupIndex: Specifies which processor is calling this function \n",
    "\n",
    "# NumPageGroup: Number of the pages that was given to each processor\n",
    "\n",
    "# Start: Start the downloading from which page? Start = 50 -> Starting extracting the HTML code of the anime \n",
    "# in the page 50\n",
    "\n",
    "def GetHTML(URLS, GroupIndex, NumPageGroup, Start):\n",
    "    \n",
    "    # Going through each page\n",
    "    for index, ListURLs in enumerate(URLS):\n",
    "        \n",
    "        # If the page number that we are considering at this moment lower than Start, we will ignore it\n",
    "        if index < Start:\n",
    "            continue\n",
    "            \n",
    "        # For each page we create a new folder to store its anime HTML code in it. \n",
    "        FolderName = 'page' + str(GroupIndex*NumPageGroup + index +1)\n",
    "        Path = os.path.join(os.getcwd(),'HTMLS', FolderName)\n",
    "        os.mkdir(Path)\n",
    "        \n",
    "        # Iterating over all the anime URL in a specific page\n",
    "        for i, url in enumerate(ListURLs):\n",
    "            \n",
    "            # Request to get the webpage of that anime's URL\n",
    "            AnimePage = requests.get(url)\n",
    "            \n",
    "            # Get the HTML code of that webpage\n",
    "            AnimePageSoup = BeautifulSoup(AnimePage.content, 'html.parser')\n",
    "            \n",
    "            # Create a html file to store the HTML code of the webpage in it\n",
    "            file = open(Path + '/' + 'anime_' + str(i) + '.html',\"w\")\n",
    "            file.write(str(AnimePageSoup))\n",
    "            file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacf63c",
   "metadata": {},
   "source": [
    "### 1.2.4 Parallelizing the Crawling process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc688f2",
   "metadata": {},
   "source": [
    "So first we check how many processors we have in the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c621f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of processors: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of processors:\", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590ada1",
   "metadata": {},
   "source": [
    "So in this case we should distribute our URLS over these processors. We read all the URLs that we have stored in the 'URLs.txt' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead245d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllURLs = []\n",
    "with open('URLs.txt') as f:\n",
    "    for row in f:\n",
    "        AllURLs.append(row.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4a437",
   "metadata": {},
   "source": [
    "Now we want to check how many URLs we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d2a928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of URLs is: 19128\n"
     ]
    }
   ],
   "source": [
    "print('The total number of URLs is:', len(AllURLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf4889",
   "metadata": {},
   "source": [
    "* **Note:** We know that we should store all the anime's webpage HTML code in a single folder for the ones that have been appeared in the same wepbage, so all of these URLs should be entirely given to a processor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfaeeff",
   "metadata": {},
   "source": [
    "* **Note:** We know that we extracted the URLs in order, so if we take the URLs 50 by 50 we will have the anime URLs that are in the same page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60848ee4",
   "metadata": {},
   "source": [
    "Now we group each 50 URLs to construct the pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3730c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EachPageURLs = [AllURLs[i*50:(i+1)*50] for i in range(len(AllURLs)//50+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c82bf6",
   "metadata": {},
   "source": [
    "Here we will take a look at how many pages that we have here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "372960e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of webpages in the website: 383\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of webpages in the website:\", len(EachPageURLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59759467",
   "metadata": {},
   "source": [
    "Now we check how many pages should be given to each of the processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "7d143748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages to be given to each processor:  96\n"
     ]
    }
   ],
   "source": [
    "NumOfPage = ceil(ceil(len(AllURLs)/50)/mp.cpu_count())\n",
    "print('Number of pages to be given to each processor: ', NumOfPage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad70465",
   "metadata": {},
   "source": [
    "* Each processor is responsible to download the anime's HTML code that are in 96 pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8202b99",
   "metadata": {},
   "source": [
    "Now we group the pages that should be given to each of the processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700c5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupPage = [EachPageURLs[i*NumOfPage:(i+1)*NumOfPage] for i in range(mp.cpu_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b846a4",
   "metadata": {},
   "source": [
    "Let's check the number of pages in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508216ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 96, 96, 95]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(len , GroupPage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad966b",
   "metadata": {},
   "source": [
    "As we can see all the processors have been assigned 96 pages to download except the last one. We remind that we have 383 page which couldn't be distributed evenly between all the processors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31568fe2",
   "metadata": {},
   "source": [
    "### Downloading HTMLs in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79014bf9",
   "metadata": {},
   "source": [
    "As we saw before, we have 4 processors in the system and we want to distribute the work among these 4 processors.<br/>\n",
    "We will give a subsets of pages to each of these processors to download their anime HTMl code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9a861a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many processors we want to pool handle \n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# Call the GetHTML function for each of the processors given their assigned pages\n",
    "results = [pool.apply_async(GetHTML, args = (GroupPage[i],i, NumOfPage, 5)) for i in range(mp.cpu_count())]\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51699a6e",
   "metadata": {},
   "source": [
    "# 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007d5e3",
   "metadata": {},
   "source": [
    "Now in this question we should produce a .tsv file for each anime which contains some information extracted from their webpage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60100bf2",
   "metadata": {},
   "source": [
    "In order to do this, first we check how we can get the information for just one anime and then we will expand the idea to other anime that we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791438b",
   "metadata": {},
   "source": [
    "The information that we should extract for each anime are: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58e960",
   "metadata": {},
   "source": [
    "1- **Anime Name** (to save as animeTitle): String <br/>\n",
    "2- **Anime Type** (to save as animeType): String<br/>\n",
    "3- **Number of episode** (to save as animeNumEpisode): Integer<br/>\n",
    "4- **Release and End Dates of anime** (to save as releaseDate and endDate): Convert both release and end date into datetime format.<br/>\n",
    "5- **Number of members** (to save as animeNumMembers): Integer<br/>\n",
    "6- **Score** (to save as animeScore): Float<br/>\n",
    "7- **Users** (to save as animeUsers): Integer<br/>\n",
    "8- **Rank** (to save as animeRank): Integer<br/>\n",
    "9- **Popularity** (to save as animePopularity): Integer<br/>\n",
    "10- **Synopsis** (to save as animeDescription): String<br/>\n",
    "11- **Related Anime** (to save as animeRelated): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.<br/>\n",
    "12- **Characters** (to save as animeCharacters): List of strings.<br/>\n",
    "13- **Voices** (to save as animeVoices): List of strings<br/>\n",
    "14- **Staff** (to save as animeStaff): Include the staff name and their responsibility/task in a list of lists.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5adca0",
   "metadata": {},
   "source": [
    "* For getting each of the information that we need, we write a function to have a better understanding of how we should extract each of these information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49d395",
   "metadata": {},
   "source": [
    "### 1.3.1 Function AnimeName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740b0f2",
   "metadata": {},
   "source": [
    "We know that the name of the anime can be extracted from the title of the webpage. So we send the html code of the webpage to this function, and we will return the name of the anime extracted from the title of the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb7925fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAnimeName(webpage):\n",
    "    return webpage.title.getText().strip().replace(' - MyAnimeList.net', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f197588",
   "metadata": {},
   "source": [
    "### 1.3.2 Function GetAnimeType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f6c2c",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7eb45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAnimeType(Tag):\n",
    "    Temp = Tag.getText().split()\n",
    "    return Temp[-1] if len(Temp) >1 else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7cbdc",
   "metadata": {},
   "source": [
    "### 1.3.3 Function Episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672faa7",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9e85e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNumOfEpisode(Tag):\n",
    "    Temp = Tag.getText().strip().split()[-1]\n",
    "    return int(Temp) if Temp.isdigit() else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba1c47",
   "metadata": {},
   "source": [
    "### 1.3.4 Function DateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a0ebc",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime. <br/><br/>\n",
    "* Some of the animes may have just the release date. So we should be careful about this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "853481f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDates(Tag):\n",
    "    Release, End = '', ''\n",
    "    Temp = Tag.getText().strip().replace('Aired:\\n  ', '').split('to')\n",
    "    Release = Temp[0] if len(Temp) else ''\n",
    "    End = Temp[1] if len(Temp) == 2 else ''\n",
    "    return Release, End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc18dd0",
   "metadata": {},
   "source": [
    "### 1.3.5 Function Members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21b155",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa01c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMembers(Tag):\n",
    "    return int(Tag.getText().replace('\\n', '').split()[1].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fd01e",
   "metadata": {},
   "source": [
    "### 1.3.6 Function ScoreAndUsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b78920",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0241caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetScoreAndUsers(Tag):\n",
    "    Rank = re.findall('[0-9|,]+ users', str(Tag))\n",
    "    RankValue = int(Rank[0].split()[0].replace(',', '')) if len(Rank) else ''\n",
    "    Score = re.findall('Score:[0-9|.]+', Tag.getText().replace(\"\\n\", ''))\n",
    "    ScoreValue = float(Score[0].split(\":\")[1]) if len(Score) else ''\n",
    "    return ScoreValue, RankValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e7525",
   "metadata": {},
   "source": [
    "### 1.3.7 Function Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48170a3",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90eaa92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRank(Tag):\n",
    "    Temp = re.findall('#[0-9|,]+', str(Tag))\n",
    "    return int(Temp[0][1:]) if len(Temp) else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e43b5c",
   "metadata": {},
   "source": [
    "### 1.3.8 Function Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757290a",
   "metadata": {},
   "source": [
    "This information can be extracted from the values stored in a specific 'div' tag. In order do this we will send that specific tag to this function and receive the type of the anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8f196db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPopularity(Tag):\n",
    "    Temp = re.findall('#[0-9|,]+', str(Tag))\n",
    "    return int(Temp[0][1:]) if len(Temp) else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfec584",
   "metadata": {},
   "source": [
    "### 1.3.9 Function Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078f597",
   "metadata": {},
   "source": [
    "In order to get the description of the anime, there is a specific tag which can be easily identified by its 'itemprop' property. So we will give the whole html to this function and get back the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd782ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSynopsis(webpage):\n",
    "    Temp = webpage.find('p', {'itemprop': \"description\"})\n",
    "    if not Temp:\n",
    "        return ''\n",
    "    Temp = Temp.getText().replace('\\n', '')\n",
    "    return Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a54bf",
   "metadata": {},
   "source": [
    "### 1.3.10 Function Related Anime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d8537",
   "metadata": {},
   "source": [
    "To get the related animes, there is a table which its 'class' is equal to 'anime_detail_related_anime'. So to get the table we should give the function the whole html file to this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30bf71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRelatedAnime(webpage):\n",
    "    Related = webpage.find('table', {'class': \"anime_detail_related_anime\"})\n",
    "    if not Related:\n",
    "        return \"\"\n",
    "    Related = Related.find_all('a')\n",
    "    UniqueRelated = set(i.getText() for i in Related)\n",
    "    return list(UniqueRelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3adbe",
   "metadata": {},
   "source": [
    "### 1.3.11 Function Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c635727",
   "metadata": {},
   "source": [
    "We can find this information in tag 'a' that are included in a div which is its class is equal to 'detail-characters-list clearfix'. When we go for the first div that have this feature we will be received the table which is the characters and their original voices is there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95f10ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCharacters(webpage):\n",
    "    Characters = []\n",
    "    Tags = webpage.find_all('div', {'class': \"detail-characters-list clearfix\"})\n",
    "    for tag in Tags:\n",
    "        if str(tag).count('character') >1:\n",
    "            \n",
    "            # We can get the name of the character in the 'href' value of the tag 'a' property\n",
    "            AllTagA = tag.find_all('a')\n",
    "            \n",
    "            # Filter the hrefs to of the characters\n",
    "            CharactersHrefs = list(set([i['href'] for i in AllTagA if 'character' in i['href']]))\n",
    "            \n",
    "            # Filter the names of the characters\n",
    "            Characters = [i.split('/')[-1].replace('_', ' ') for i in CharactersHrefs]\n",
    "    return Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871b822",
   "metadata": {},
   "source": [
    "### 1.3.12 Function Voices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e19f7",
   "metadata": {},
   "source": [
    "With the same approach as previous, now we just extract the voices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e664b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVoices(webpage):\n",
    "    Voices = []\n",
    "    Tags = webpage.find_all('div', {'class': \"detail-characters-list clearfix\"})\n",
    "    for tag in Tags:\n",
    "        if str(tag).count('character') > 1:\n",
    "            \n",
    "            # We can get the name of the person in charge for the voice in the 'href' value in tag 'a' property\n",
    "            AllTagA = tag.find_all('a')\n",
    "            \n",
    "            # Filter the hrefs for voices\n",
    "            VoicesHrefs = list(set([i['href'] for i in AllTagA if 'people' in i['href']]))\n",
    "            \n",
    "            # Filter the names of the people\n",
    "            Voices = [i.split('/')[-1].replace('_', ' ') for i in VoicesHrefs]\n",
    "    return Voices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bd8ed",
   "metadata": {},
   "source": [
    "### 1.3.13 Function Staff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90395d78",
   "metadata": {},
   "source": [
    "Here in this function we will look for the other div tha its 'class' is equal to 'detail-characters-list clearfix'. Then the name of the staff can be extracted from 'img' tags and also their duties can be found in the 'small' tags which are in the same 'row' as their image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a628582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStaff(webpage):\n",
    "    StaffDuty = []\n",
    "    Tags = webpage.find_all('div', {'class': \"detail-characters-list clearfix\"})\n",
    "    for tag in Tags:\n",
    "        if str(tag).count('character') == 1:\n",
    "            Staff =  tag.find_all('tr')\n",
    "            for i in Staff:\n",
    "                NewStaff = [i.find('a')['href'].split('/')[-1].replace('_', ' ')] # Extracting the name of the staff\n",
    "                Duties = list(i.find('small').getText().split(',')) # Getting the duties of the staff\n",
    "                NewStaff += [i.strip() for i in Duties]\n",
    "                StaffDuty.append(NewStaff)\n",
    "    return StaffDuty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbd662",
   "metadata": {},
   "source": [
    "### 1.3.14 Function Write to TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c4bfd",
   "metadata": {},
   "source": [
    "With the help of this function, we can write the information that we extracted from a page to its corresponded .tsv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "075d839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteToTSV(AnimeInfo, Path):\n",
    "    \n",
    "    # Open the corresponded file  \n",
    "    File = open(Path, mode = 'w')\n",
    "    \n",
    "    # Writing the header. \n",
    "    File.write(\"\\t\".join(Keys))\n",
    "    File.write('\\n')\n",
    "    \n",
    "    # Going through all the information for that specific anime\n",
    "    for i in AnimeInfo:\n",
    "        \n",
    "        # Get the string version of that value\n",
    "        STR = str(AnimeInfo[i])\n",
    "        \n",
    "        # In case an information was missing, we should put '' in the file\n",
    "        if STR == '[]':\n",
    "            STR = ''\n",
    "        \n",
    "        # Write the value of the information into the file\n",
    "        File.write(STR + ('\\t' if i!='animeStaff' else \"\" ))\n",
    "    File.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2bc03",
   "metadata": {},
   "source": [
    "### 1.3.15 Function Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a5252",
   "metadata": {},
   "source": [
    "With the help of this function we want to extract the desired information from all the .html files that we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34553752",
   "metadata": {},
   "source": [
    "Here we used some key words so we can easily get the needed information which are in some tags that share the same class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df00d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Info = ['Type:', 'Episodes:', 'Aired:', 'Members:','Score:','Ranked:', 'Popularity:']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ab353",
   "metadata": {},
   "source": [
    "Here we define the default datastructure which is a dictionary to store the inforamtion of each page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e03cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keys = ['animeTitle', 'animeType', 'animeNumEpisode', 'releaseDate', 'endDate', 'animeNumMembers'\n",
    "        , 'animeScore', 'animeUsers', 'animeRank', 'animePopularity', 'animeDescription', 'animeRelated',\n",
    "        'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "# Empty instance for storing the information for each anime\n",
    "MyAnimeInfo = {i:\"\" for i in Keys} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745991de",
   "metadata": {},
   "source": [
    "Here we will go through each HTML file that we have just downloaded. Then we extract the infomration that we need from the HTML file and then store them in a .tsv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb86c4",
   "metadata": {},
   "source": [
    "* **Note:** as we wanted to avoid each processor override the datastructure of another processor we gave each processor its own private datastructure. That is the usage of exec functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36238639",
   "metadata": {},
   "source": [
    "* exec function: Given a string it will turn that string into a python code and run the code. In this case we gave this possibility to each processor to work on its own datastructure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46254389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionExtract(ListNumPage, ProcesserNum):\n",
    "    \n",
    "    # Going through all the pages\n",
    "    for i in ListNumPage:\n",
    "        \n",
    "        # Going through all the anime in each page\n",
    "        for j in range((50 if i!= 383 else 28)):\n",
    "            \n",
    "            # Get the copy of the default datastructures\n",
    "            exec('NewAnime'+str(ProcesserNum)+'= MyAnimeInfo.copy()')\n",
    "            \n",
    "            # Reading the .html file of a specific anime\n",
    "            Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "            File = open(Path + str(i) + '/anime_' + str(j) +'.html', mode = 'r')\n",
    "            \n",
    "            # Get the parsed HTML code of the webpage of the anime \n",
    "            Webpage = BeautifulSoup(File.read(), 'html.parser')\n",
    "            \n",
    "            # Extracting some of the information that have been stored in the tags that have the \n",
    "            # same value as the class\n",
    "            for div in Webpage.find_all('div', {'class':\"spaceit_pad\"}):\n",
    "                if Info[0] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeType'] = GetAnimeType(div)\")\n",
    "                elif Info[1] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeNumEpisode'] = GetNumOfEpisode(div)\")\n",
    "                elif Info[2] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['releaseDate'] = GetDates(div)[0]\")\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['endDate'] = GetDates(div)[1]\")\n",
    "                elif Info[3] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeNumMembers'] = GetMembers(div)\")\n",
    "                elif Info[4] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeScore'] = GetScoreAndUsers(div)[0]\")\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeUsers'] = GetScoreAndUsers(div)[1]\")\n",
    "                elif Info[5] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animeRank'] = GetRank(div)\")\n",
    "                elif Info[6] in str(div):\n",
    "                    exec('NewAnime'+ str(ProcesserNum) +\"['animePopularity'] = GetPopularity(div)\")\n",
    "            \n",
    "            # Extracting the other information \n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeTitle'] = GetAnimeName(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeDescription'] = GetSynopsis(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeRelated'] = GetRelatedAnime(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeCharacters'] = GetCharacters(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeVoices'] = GetVoices(Webpage)\")\n",
    "            exec('NewAnime'+ str(ProcesserNum) +\"['animeStaff'] = GetStaff(Webpage)\")\n",
    "            \n",
    "            TSVPath = Path + str(i) + '/anime_' + str(j) +'.tsv'\n",
    "            exec('WriteToTSV(NewAnime'+ str(ProcesserNum)+',TSVPath)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53e121",
   "metadata": {},
   "source": [
    "### Creating the .tsv files in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe78910",
   "metadata": {},
   "source": [
    "* **In order to speed up the process, we will distribute the work among the available CPUs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f02ae8",
   "metadata": {},
   "source": [
    "We should give a subset of pages to each CPU to make its animes' .tsv file. <br/>\n",
    "Here we will group the page numbers that should be given to each processor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa13be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "RangePage = list(range(1, len(EachPageURLs) + 1))\n",
    "PageNums = [RangePage[i * NumOfPage:(i+1) * NumOfPage] for i in range(mp.cpu_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03391175",
   "metadata": {},
   "source": [
    "Call the function for each CPU give a susbset of .html files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c0d68296",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = [pool.apply_async(FunctionExtract, args = (PageNums[i],i)) for i in range(mp.cpu_count())]\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329d0e4",
   "metadata": {},
   "source": [
    "# 2. Search Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ff9c1",
   "metadata": {},
   "source": [
    "Here we are asked to build a search engine which given a query, will give back the documents that are similar to the given query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da72a3",
   "metadata": {},
   "source": [
    "### 2.0 Pre-processing the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93de6a6",
   "metadata": {},
   "source": [
    "Here we will pre-process all the information of an anime and store it to another .tsv file which will be name SynopsisPrepAnime_(i).csv which i corresponds to the index number of the page in its page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8bdaa",
   "metadata": {},
   "source": [
    "To do pre-processing we have 5 stages and for each stage we will write a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca972cc",
   "metadata": {},
   "source": [
    "### 2.0.1 Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36c4c4",
   "metadata": {},
   "source": [
    "Given a string, we will return the words that are in the given string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd722bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this functions to extract the terms in each given sentence \n",
    "def Tokenization(Sentence):\n",
    "    return nltk.word_tokenize(Sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe887d36",
   "metadata": {},
   "source": [
    "### 2.0.2 Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368181a",
   "metadata": {},
   "source": [
    "Given a list of strings, we will return the same strings but in lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20bdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all the words that we have in the given list of words to their lowercase\n",
    "def Lowercasing(Words):\n",
    "    return [w.lower() for w in Words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f0b34",
   "metadata": {},
   "source": [
    "### 2.0.3 StopWordsRemoval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a8a51",
   "metadata": {},
   "source": [
    "Here we will define a list which contains all the stopwords in English language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01cd6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74986fc2",
   "metadata": {},
   "source": [
    "Given a list of strings, we will remove the stopwords from that strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa662c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will consider just those terms that are not in stopwords\n",
    "def StopWordsRemoval(Words):\n",
    "    return [w for w in Words if w not in StopWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d02f8",
   "metadata": {},
   "source": [
    "### 2.0.4 PunctuationsRemoval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e449f",
   "metadata": {},
   "source": [
    "In this function given a list of strings, we will remove the punctuations from those strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be23c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the punctuations, we only will consider the alphabetic letters\n",
    "def PunctuationsRemoval(Words):\n",
    "    return [w for w in Words if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24313fc3",
   "metadata": {},
   "source": [
    "### 2.0.5 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccbab2",
   "metadata": {},
   "source": [
    "In this function, given a list of strings, we try to return back the stem of each string that we have in the list. <br/>\n",
    "Here we will use 'PorterStemmer' algorithm to do stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e437df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stem of each word will be returned \n",
    "def Stemming(Words):\n",
    "    return [PorterStemmer().stem(w) for w in Words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68d729",
   "metadata": {},
   "source": [
    "* Here we are asked to work with the 'Synopsis' of each anime, so we decided pre-process only the synopsis of each anime and not all the information that we extracted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14233c22",
   "metadata": {},
   "source": [
    "* **Note:** For your information, we want to do the pre-process we will distribute the work among the available number of CPUs in the system. Each CPU will be given a subset of pages to do the pre-process for the contained anime information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac440f57",
   "metadata": {},
   "source": [
    "### 2.0.6 Main Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0a5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MainFunction(Pages):\n",
    "    \n",
    "    # For each page \n",
    "    for page in Pages:\n",
    "        \n",
    "        # For each anime in a specific page\n",
    "        for anime in range((50 if page != 383 else 28)):\n",
    "            \n",
    "            # Read the '.tsv' file of that specific anime\n",
    "            Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "            File = open(Path + str(page) + '/anime_' + str(anime) +'.tsv', mode = 'r')\n",
    "            \n",
    "            # For each anime we want to extract the synopsis \n",
    "            Data = File.read().split('\\n')[1].split('\\t')[10]\n",
    "            File.close()\n",
    "            \n",
    "            # Given the synopsis of a specific anime to pre-processing stage\n",
    "            Tokens = Tokenization(Data)\n",
    "            Lowercase = Lowercasing(Tokens)\n",
    "            WithoutStop = StopWordsRemoval(Lowercase)\n",
    "            WithoutPuncs = PunctuationsRemoval(WithoutStop)\n",
    "            Stems = Stemming(WithoutPuncs)\n",
    "            \n",
    "            # Store the result of the pre-processing stage in a '.csv' file for each specific anime\n",
    "            PreProcessed = open(Path + str(page) + '/anime_' + str(anime) +'_synopsisPrep.csv', mode = 'w')\n",
    "            PreProcessed.write(\",\".join(Stems))\n",
    "            PreProcessed.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2948ff",
   "metadata": {},
   "source": [
    "## 2.1 Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1835a54",
   "metadata": {},
   "source": [
    "Here we will pre-process the synopsis of each anime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0232e",
   "metadata": {},
   "source": [
    "Group the pages to be given to the CPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "43338b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RangePage = list(range(1, len(EachPageURLs) + 1))\n",
    "PageNums = [RangePage[i * NumOfPage:(i+1) * NumOfPage] for i in range(mp.cpu_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02733231",
   "metadata": {},
   "source": [
    "Pre-processing all the synopsis of animes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6fc0f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = [pool.apply_async(MainFunction, args = (PageNums[i],)) for i in range(mp.cpu_count())]\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40485e",
   "metadata": {},
   "source": [
    "## 2.1.1 Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee39678",
   "metadata": {},
   "source": [
    "Here we will create a file name **'vocabulary.json'** which will map each word that we want to take into account for our analysis to a specific ID. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da3dd0",
   "metadata": {},
   "source": [
    "We will go through all the pre-processed files that we have created for each anime and extract all the words in them and give each word a specific ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81d85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary which stores each words in all anime \n",
    "MyDict = dict()\n",
    "\n",
    "# For each page\n",
    "for i in range(1, 384):\n",
    "    \n",
    "    # For each anime\n",
    "    for j in range((50 if i != 383 else 28)):\n",
    "        \n",
    "        # Open the '.csv' file of each specific anime\n",
    "        Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "        File = open(Path + str(i) + '/anime_' + str(j) +'_synopsisPrep.csv', mode = 'r')\n",
    "        \n",
    "        # Get the words in the synopsis of each anime\n",
    "        DescriptionWord = File.read().split(',')\n",
    "        \n",
    "        # For each word in the synopsis we will have an entry in the dictionary\n",
    "        for term in DescriptionWord:\n",
    "            MyDict[term] = 0 \n",
    "            \n",
    "# Assign each word a unique id\n",
    "for term_id,term  in enumerate(MyDict.keys()):\n",
    "    MyDict[term] = term_id\n",
    "\n",
    "# Writing the resulted information into a .json file so we can easily retrieve them\n",
    "with open(\"Vocabulary.json\", \"w\") as write_file:\n",
    "    json.dump(MyDict, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3684f",
   "metadata": {},
   "source": [
    "### 2.1.1.1 Creating Inverted Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd634a6",
   "metadata": {},
   "source": [
    "Here we will go through all the preprocessed files and add the documents id to the values of the terms we find in each document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0efa6",
   "metadata": {},
   "source": [
    "We will store the document ids as a combination of page number of anime id (#page, #anime_id)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45ebef",
   "metadata": {},
   "source": [
    "* We will store our created inverted index into a file named **'InvertedIndex_1'**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ab8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary which have all the terms in the corpus and their term_id\n",
    "Vocabularies = json.load(open(\"Vocabulary.json\", \"r\"))\n",
    "\n",
    "# We will store our inverted index in this dictionary\n",
    "# Keys: term\n",
    "# Value: a list of document_id (which is a pair of page and anime index) \n",
    "InvertedIndexes = defaultdict(set)\n",
    "\n",
    "# For each page \n",
    "for i in range(1, 384):\n",
    "    # For each anime \n",
    "    for j in range((50 if i != 383 else 28)):\n",
    "        \n",
    "        # Read the '.csv' file for each specific anime\n",
    "        Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "        File = open(Path + str(i) + '/anime_' + str(j) +'_synopsisPrep.csv', mode = 'r')\n",
    "        \n",
    "        # Extract the words in the synopsis for each anime\n",
    "        DescriptionWord = File.read().split(',')\n",
    "        \n",
    "        # For all the terms that are in this anime, we will add the document_id to its list\n",
    "        for term in DescriptionWord:\n",
    "            InvertedIndexes[Vocabularies[term]].add((i,j))\n",
    "            \n",
    "# Sorting the document_ids\n",
    "for term in InvertedIndexes:\n",
    "    InvertedIndexes[term] = sorted(InvertedIndexes[term])\n",
    "    \n",
    "# We will store these information in a '.json' file so we can easily retrieve them\n",
    "with open(\"InvertedIndex_1.json\", \"w\") as write_file:\n",
    "    json.dump(InvertedIndexes, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570c66c",
   "metadata": {},
   "source": [
    "## 2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f851d",
   "metadata": {},
   "source": [
    "Here given a query we want to return the documents that are containing all the words in the given query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333b0a0",
   "metadata": {},
   "source": [
    "In order to answer the query, first we should pre-process the query as well. We need to do this as we have pre-processed all the information that we have extracted from the htmls. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca507ee",
   "metadata": {},
   "source": [
    "* Here we will write a function which given a string, it will pre-process the string and return the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf6e10",
   "metadata": {},
   "source": [
    "### 2.1.2.1 Function String Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5107aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a stirng, the string will go to the pre-process stage\n",
    "def StringPreProcess(string):\n",
    "    Tokens = Tokenization(string)\n",
    "    LowerCased = Lowercasing(Tokens)\n",
    "    WithoutStop = StopWordsRemoval(LowerCased)\n",
    "    WithoutPunc = PunctuationsRemoval(WithoutStop)\n",
    "    Stemmed = Stemming(WithoutPunc)\n",
    "    return Stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e925f",
   "metadata": {},
   "source": [
    "### 2.1.2.2 Function token to TermID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c505d",
   "metadata": {},
   "source": [
    "This function given a token and the Vocabulary data structure will return back the term_id of that token. If the token is not in the Vocabulary, this function will return None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1966f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordToTermID(Token, Vocabulary):\n",
    "    return Vocabulary.get(Token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f122e9",
   "metadata": {},
   "source": [
    "### 2.1.2.3 Function Extracting Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e52711",
   "metadata": {},
   "source": [
    "Given a TermID and inverted indexes, this function will return the documents which are containing this specific token. If TermID is equal to 'None', this function will return empty list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9151b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractDocs(TermId, InvertedIndex):\n",
    "    return InvertedIndex.get(str(TermId)) if TermId else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e299d",
   "metadata": {},
   "source": [
    "### 2.1.2.4 Function Query to Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c52e5d",
   "metadata": {},
   "source": [
    "This function will process the given query and return back the documents that are containing all the words in the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d60b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryToDocuments(Query, Vocabulary, InvertedIndex):\n",
    "    PreProcessedQuery = StringPreProcess(Query)\n",
    "    \n",
    "    # If the query after the pre-process stage was empty, return an empty list\n",
    "    if not len(PreProcessedQuery):\n",
    "        return []\n",
    "    \n",
    "    # This list will store all the documents in which each term was there. \n",
    "    Documents = []\n",
    "\n",
    "    # Convert each term to its termid\n",
    "    TermIDs = [WordToTermID(term, Vocabulary) for term in PreProcessedQuery]\n",
    "    \n",
    "    # All the documents for each Term\n",
    "    Documents = [set(map(tuple, ExtractDocs(TermID, InvertedIndex))) for TermID in TermIDs]\n",
    "    \n",
    "    # Take the intersection of the documents\n",
    "    Result = sorted(Documents[0].intersection(*Documents))\n",
    "    return Result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4ce5e",
   "metadata": {},
   "source": [
    "### 2.1.2.5 Function Data to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a12f3",
   "metadata": {},
   "source": [
    "This function given the name of the columns and the values for each column will create a dataframe with those information which help to visualize the results in tabular format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67500e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the column name and the records, will produce a dataframe\n",
    "def DataToDataFrame(ColumnsName, Records):\n",
    "    \n",
    "    # Declare a datafram with specific column name\n",
    "    Table = pd.DataFrame(columns = ColumnsName)\n",
    "    \n",
    "    # For each records \n",
    "    for rec in Records:\n",
    "        \n",
    "        # We will store the values in each records in a dictionary as for adding a record to the dataframe\n",
    "        # we should pass a dictionary\n",
    "        TempDict = dict()\n",
    "        \n",
    "        # For each value of the columns of the records\n",
    "        for col in range(len(ColumnsName)):\n",
    "            \n",
    "            # Give the value of that column to the dictionary\n",
    "            TempDict[ColumnsName[col]] = rec[col]\n",
    "            \n",
    "        # Add this new record to the dataframe\n",
    "        Table = Table.append(TempDict,ignore_index = True )\n",
    "    display(Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07757869",
   "metadata": {},
   "source": [
    "### 2.1.2.6 Function Main Answer Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "8f83871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query here: saiyan race\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Kai (Dragon Ball Z Kai)</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "0                                      Dragon Ball Z   \n",
       "1                           Dragon Ball Super: Broly   \n",
       "2                Dragon Ball Kai (Dragon Ball Z Kai)   \n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "0  Five years after winning the World Martial Art...   \n",
       "1  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "2  Five years after the events of Dragon Ball, ma...   \n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "\n",
       "                                                 URL  \n",
       "0    https://myanimelist.net/anime/813/Dragon_Ball_Z  \n",
       "1  https://myanimelist.net/anime/36946/Dragon_Bal...  \n",
       "2  https://myanimelist.net/anime/6033/Dragon_Ball...  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading Vocabulary\n",
    "Vocabulary = json.load(open(\"Vocabulary.json\", \"r\"))\n",
    "\n",
    "# Loading Inverted Indexes\n",
    "InvertedIndex = json.load(open(\"InvertedIndex_1.json\", 'r'))\n",
    "\n",
    "Query = input('Please enter your query here: ')\n",
    "\n",
    "# Results \n",
    "Results = QueryToDocuments(Query, Vocabulary, InvertedIndex)\n",
    "\n",
    "# To pass the data to show in tabular format \n",
    "Columns, Data = ['animeTitle', 'animeDescription', 'URL'], []\n",
    "\n",
    "# Now here we should iterate over all the results and retrieve the information for each of these anime\n",
    "for Res in Results:\n",
    "    Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "    FileURL = open('URLs.txt', mode = 'r')\n",
    "    FileInfo = open(Path + str(Res[0]) + '/anime_' + str(Res[1]) +'.tsv', mode = 'r')\n",
    "\n",
    "    Information = FileInfo.read().split('\\n')[1].split('\\t')\n",
    "    Title = Information[0]\n",
    "    Description = Information[10]\n",
    "    URL = FileURL.read().split('\\n')[(Res[0]-1)*50 + Res[1]]\n",
    "    Data.append([Title, Description, URL])\n",
    "\n",
    "# Showing the results\n",
    "DataToDataFrame(Columns, Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1980b56",
   "metadata": {},
   "source": [
    "## 2.2. Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da72a6",
   "metadata": {},
   "source": [
    "For this part we are asking to given a query, show the top k documents that are more closer to this query using scoring schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac5056",
   "metadata": {},
   "source": [
    "To answer this question, first we should go through some definitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993edf1b",
   "metadata": {},
   "source": [
    "### - What is tf?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755bbc0e",
   "metadata": {},
   "source": [
    "We define tf(t, d) of term t in documnet d as the **division** of the **number of occurance of term t in document d** by **the total number of occurance of term t in all documents** in the corpus. You can see the formula here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cbca7",
   "metadata": {},
   "source": [
    "### - What is idf?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90719e5c",
   "metadata": {},
   "source": [
    "Idf (inverse document frequency) is a measure of how much information the word provides. This value will be obtained by **dividing** the **total number of documents** by the **number of documents containing the term**, and then taking the **logarithm** of that quotient. You can see the formula here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002d766",
   "metadata": {},
   "source": [
    "### - What is tfidf?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7720815",
   "metadata": {},
   "source": [
    "Tfidf for each term and document can be obtained by the **multiplication** of **tf(t,d) and idf(t,D)**. The higher this value is, the more important that term is in that specific document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b379d",
   "metadata": {},
   "source": [
    "## 2.2.1 Creating the inverted indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e245d77",
   "metadata": {},
   "source": [
    "Here we should build an inverted index in the format that each term corresponds to the documents in which that term is appearing and also the tfidf of that term in that specific document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4c70c",
   "metadata": {},
   "source": [
    "Here we will define some functions to help us to do this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb181bbd",
   "metadata": {},
   "source": [
    "### 2.2.1.1 Function tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa492ad2",
   "metadata": {},
   "source": [
    "This function given the list of documents and the number of occurance of each specific term T in each of these documents and also the number of all the documents in the corpus, will return the terms and its tfidf for each of the documents that are containing that term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb46cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTfIdf(invertedIndex, NumberOfDocs):\n",
    "    for Term in invertedIndex: # For each term\n",
    "        \n",
    "        # How many times that term occured in the corpus\n",
    "        TotalOccurance = sum(doc[1] for doc in invertedIndex[Term])\n",
    "        \n",
    "        # How many documents are containig that term \n",
    "        NumberOfTDocs = len(invertedIndex[Term])\n",
    "        \n",
    "        # Computing the idf of a term \n",
    "        idf = log(NumberOfDocs / NumberOfTDocs)\n",
    "        \n",
    "        for docs in invertedIndex[Term]: # For each document that is contaning the term \n",
    "            \n",
    "            # Put the tfidf (tf*idf) instead of the number of occurence of that term in the document\n",
    "            docs[1] = (docs[1] / TotalOccurance) * idf\n",
    "            \n",
    "    return invertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633bf81",
   "metadata": {},
   "source": [
    "### 2.2.1.2 Create a suitable inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7a07",
   "metadata": {},
   "source": [
    "Now we should write a code that build another type of inverted index, which each term corresponds to the documents that are contaning that term and also the number of occurence of the term in each of those documents. This inverted index will be saved in 'InvertedIndex_2.json' file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bee6d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the term_id of each term\n",
    "Vocabularies = json.load(open(\"Vocabulary.json\", \"r\"))\n",
    "\n",
    "# To store the inverted index\n",
    "# We will store our inverted index in this dictionary\n",
    "# Keys: term\n",
    "# Value: a list of (document_id (which is a pair of page and anime index), #occurance of the term in this document)\n",
    "InvertedIndexes = defaultdict(set)\n",
    "\n",
    "# For each page\n",
    "for i in range(1, 384):\n",
    "    # For each anime \n",
    "    for j in range((50 if i != 383 else 28)):\n",
    "        \n",
    "        # Read the '.csv' file for a specific anime\n",
    "        Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "        File = open(Path + str(i) + '/anime_' + str(j) +'_synopsisPrep.csv', mode = 'r')\n",
    "        \n",
    "        # Extract the words in the synopsis of that anime\n",
    "        DescriptionWord = File.read().split(',')\n",
    "        \n",
    "        # Check the number of the occurances of each word in the synopsis of a specific anime\n",
    "        NumberOfOccurenceInDoc = Counter(DescriptionWord)\n",
    "        \n",
    "        # For each term we will add this anime and its total number of occurance in this anime\n",
    "        for term in NumberOfOccurenceInDoc:\n",
    "            InvertedIndexes[Vocabularies[term]].add(((i,j), NumberOfOccurenceInDoc[term]))\n",
    "        \n",
    "        \n",
    "        \n",
    "# Sorting the document ids\n",
    "for term in InvertedIndexes:\n",
    "    InvertedIndexes[term] = sorted(InvertedIndexes[term])\n",
    "    \n",
    "# Store this inverted index in '.json' file to get the information pretty easily\n",
    "with open(\"InvertedIndex_2.json\", \"w\") as write_file:\n",
    "    json.dump(InvertedIndexes, write_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85bff7",
   "metadata": {},
   "source": [
    "Now we are able to compute the tf, idf and ea the end the tfidf of each term and docment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e8679",
   "metadata": {},
   "source": [
    "### 2.2.1.3 Main Creating the new  Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a16de3",
   "metadata": {},
   "source": [
    "Now here we will create a new inverted index such that each term has the information about each document that is containig that specific term and also the tfidf of that term and that specific document. This new inverted index will be saved in a file named **'InvertedIndex_3.json'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddf637ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the second inverted index term -> documents, # Occurence\n",
    "InvertedIndex = json.load(open(\"InvertedIndex_2.json\", \"r\"))\n",
    "\n",
    "# Number of the docuements in the corpus \n",
    "NumOfDocs = len(AllURLs) # Number of documents are the number of the anime\n",
    "\n",
    "# Computing the tfidf for each term and document\n",
    "NewInvertedIndex = GetTfIdf(InvertedIndex, NumOfDocs)\n",
    "\n",
    "# Saving the new inverted index into a new file named 'InvertedIndex_3.json'\n",
    "with open(\"InvertedIndex_3.json\", \"w\") as write_file:\n",
    "    json.dump(NewInvertedIndex, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef9ad1a",
   "metadata": {},
   "source": [
    "### 2.2.1.4 Function query to tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30de27",
   "metadata": {},
   "source": [
    "This function given the query, the second inverted index which each term corresonds to the docuemnts that are containig the number of occurence of that term in those documents, the vocabulary, and the number of all the docuemnts in the corpus will compute the tfidf of the term in the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a81785eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryToTfidf(Query, invertedIndex,Vocabulary,  NumberOfDocs):\n",
    "    \n",
    "    \n",
    "    # Mapping the words in query to their term_id\n",
    "    PreProcessedQuery = StringPreProcess(Query)\n",
    "    Query = [str(Vocabulary.get(Term)) for Term in PreProcessedQuery]\n",
    "    \n",
    "    # Check the number of occurence of each term in the query \n",
    "    QueryDict = Counter(Query)\n",
    "    \n",
    "    for Term in QueryDict:# for each term in the query\n",
    "        \n",
    "        # If that term is in the inverted index\n",
    "        if invertedIndex.get(Term):\n",
    "\n",
    "            # How many times that term occured in the corpus\n",
    "            TotalOccurance = sum(doc[1] for doc in invertedIndex[Term])\n",
    "\n",
    "            # How many documents are containig that term \n",
    "            NumberOfTDocs = len(invertedIndex[Term])\n",
    "\n",
    "            # Computing the idf of a term \n",
    "            idf = log(NumberOfDocs / NumberOfTDocs)\n",
    "\n",
    "            # Put the tfidf (tf*idf) instead of the number of occurence of that term in the query\n",
    "            QueryDict[Term] = (QueryDict[Term]/ TotalOccurance) * idf\n",
    "        else:\n",
    "            QueryDict[Term] = 0\n",
    "   \n",
    "    return QueryDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd13c1",
   "metadata": {},
   "source": [
    "### 2.2.1.5 Function Dot Product "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8cafa",
   "metadata": {},
   "source": [
    "In this function given two vectors, we will compute the dot product of the two and return back the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4190dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProduct(V1, V2): # The two given vectors should have the same length\n",
    "    return sum(V1[i] * V2[i] for i in range(len(V1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15b2ce",
   "metadata": {},
   "source": [
    "### 2.2.1.6 Function Euclidean norm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58925ec3",
   "metadata": {},
   "source": [
    "This function given a vector, will compute the Euclidean norm of that vector and return the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18617a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the Eucledean norm the given vector\n",
    "def Euclidean(Vector): \n",
    "    return (sum(Vector[i]**2 for i in range(len(Vector)))) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c771d",
   "metadata": {},
   "source": [
    "### 2.2.1.7 Function Query to Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d35041",
   "metadata": {},
   "source": [
    "In this function, given the query and the 3rd inverted index, we will extract the documents that are contaning all the terms in the query. The result will be a dictinoary which the keys are the documents and the keys are a list which is containing all the tfirdf of the terms in the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3a15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryToDocuemnts(Query, invertedIndex):\n",
    "    \n",
    "    AllTermsDict = []\n",
    "    for term in Query: # For each term in the query\n",
    "        TermDict = {} # The dictionary which contains the documents that are containing that term \n",
    "        \n",
    "        # For each document which is contaning the term \n",
    "        if invertedIndex.get(term):\n",
    "            for doc in invertedIndex[term]:\n",
    "\n",
    "                # Add the document and the corresponded tfidf for that term in that document\n",
    "                TermDict[tuple(doc[0])] = doc[1]\n",
    "            # Add the dictionary for a specific term in the query which is containig all \n",
    "            # the document that are containig that term \n",
    "\n",
    "        AllTermsDict.append(TermDict.copy())\n",
    "    \n",
    "    # Extract the documents that are containing all the terms in the query\n",
    "    \n",
    "    # All the documents in each dictionary \n",
    "    AllDocs = [set(termDict.keys()) for termDict in AllTermsDict]\n",
    "    \n",
    "    AllIncludedDocs = AllDocs[0].intersection(*AllDocs) # Docs that are containing all the terms in the query\n",
    "    \n",
    "    # Here we are creating a dictionary which keys are the documents that are containig all the terms in the query\n",
    "    # and the keys are the tfidf of all the term in the query in sequence. \n",
    "    DocsToTfidf = defaultdict(list)\n",
    "    for termDict in AllTermsDict:\n",
    "        for doc in AllIncludedDocs:\n",
    "            DocsToTfidf[doc].append(termDict[doc])\n",
    "    \n",
    "    \n",
    "    return DocsToTfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735c905",
   "metadata": {},
   "source": [
    "### 2.2.1.8 Function Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929d082",
   "metadata": {},
   "source": [
    "In this function given the query tfidf values and all the documents that are containig all the terms in the query, we want to compute the similarity of the query with each docuement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c220f",
   "metadata": {},
   "source": [
    "* The cosine similarity between two vectores will be the **dot product of the values** in these two vectors, **divided** by **the multiplication of the euclidean norm** of each of the vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8b291ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(QueryTfidf, Documents):\n",
    "    \n",
    "    for doc in Documents:\n",
    "        \n",
    "        # Compute the dot product of tfidf of the terms in the query and a specific document \n",
    "        DotProductRes = DotProduct(QueryTfidf, Documents[doc])\n",
    "        \n",
    "        # Computer the similarity of query and a specific document \n",
    "        Similarity = DotProductRes/(Euclidean(QueryTfidf)*Euclidean(Documents[doc]))\n",
    "        \n",
    "        # Append the value of similarity at the end of the least of each document. \n",
    "        Documents[doc].append(Similarity)\n",
    "    \n",
    "    return Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2a226",
   "metadata": {},
   "source": [
    "* We are asked to sort the similar documents using heap sort. In order to use heap sort, we should have a class for the object that we are passing to the heap. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b5d3a9",
   "metadata": {},
   "source": [
    "### 2.2.1.9 Function Docs Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b6d4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docs:  \n",
    "    \n",
    "    # Storing the passed information for each object\n",
    "    def __init__(self,DocInfo):  \n",
    "        self.docInfo = DocInfo\n",
    "        \n",
    "    # Overriding the operator '<'. Here as we need the descending sort, we override in reverse. \n",
    "    def __lt__(self, other):  \n",
    "        return self.docInfo[-1] > other.docInfo[-1]\n",
    "   \n",
    "    # Overriding the operator '>'\n",
    "    def __gt__(self, other):  \n",
    "        return other.__lt__(self)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9684e52",
   "metadata": {},
   "source": [
    "### 2.2.1.10 Function HeapSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9017d",
   "metadata": {},
   "source": [
    "This function given a list of object will sort them by using heap tree and return the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "769ae6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapsort(objects):  \n",
    "    \n",
    "    # Heap Tree\n",
    "    heap = []  \n",
    "    \n",
    "    # Add all the given object to the tree\n",
    "    for element in objects:  \n",
    "        heappush(heap, element)  \n",
    "   \n",
    "    # Will store the ordered version of objects\n",
    "    ordered = []  \n",
    "    \n",
    "    # Until we have still object in tree\n",
    "    while heap:  \n",
    "        #Take the root of the tree -> Which is the highest value. \n",
    "        ordered.append(heappop(heap))  \n",
    "    \n",
    "    return ordered  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47c4ec",
   "metadata": {},
   "source": [
    "### 2.2.1.10 Main function Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4174c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query here: saiyan race\n",
      "Please enter the value of k here: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescriptio</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Kai (Dragon Ball Z Kai)</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "0                                      Dragon Ball Z   \n",
       "1  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "2                Dragon Ball Kai (Dragon Ball Z Kai)   \n",
       "3                           Dragon Ball Super: Broly   \n",
       "\n",
       "                                     animeDescriptio  \\\n",
       "0  Five years after winning the World Martial Art...   \n",
       "1  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "2  Five years after the events of Dragon Ball, ma...   \n",
       "3  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "\n",
       "                                                 Url Similarity  \n",
       "0    https://myanimelist.net/anime/813/Dragon_Ball_Z       1.00  \n",
       "1  https://myanimelist.net/anime/986/Dragon_Ball_...       1.00  \n",
       "2  https://myanimelist.net/anime/6033/Dragon_Ball...       1.00  \n",
       "3  https://myanimelist.net/anime/36946/Dragon_Bal...       1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the query from the user\n",
    "Query = input('Please enter your query here: ')\n",
    "\n",
    "# First K most similar documents\n",
    "K = int(input('Please enter the value of k here: '))\n",
    "\n",
    "# Loading the second inverted index term -> documents, # Occurence\n",
    "SecondInvertedIndex = json.load(open(\"InvertedIndex_2.json\", \"r\"))\n",
    "\n",
    "# Loading the third inverted index term -> documents, # tfidf\n",
    "ThirdInvertedIndex = json.load(open(\"InvertedIndex_3.json\", \"r\"))\n",
    "\n",
    "# Loading the vocabulary file which maps each term to a specific ID\n",
    "Vocabularies = json.load(open(\"Vocabulary.json\", \"r\"))\n",
    "\n",
    "# Compute the TFIDF of the query\n",
    "QueryTFIDF = QueryToTfidf(Query, SecondInvertedIndex,Vocabularies, len(AllURLs))\n",
    "\n",
    "# Take the documents that are containing all the words\n",
    "RelatedDocuments = QueryToDocuemnts(QueryTFIDF.keys(), ThirdInvertedIndex)\n",
    "\n",
    "# Compute the similarity of each document with query\n",
    "DocumentsSimilarity = Similarity(list(QueryTFIDF.values()), deepcopy(RelatedDocuments))\n",
    "\n",
    "# Here we will store the details of each similar documents \n",
    "Data = []\n",
    "for Res in DocumentsSimilarity:\n",
    "    Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "    FileURL = open('URLs.txt', mode = 'r')\n",
    "    FileInfo = open(Path + str(Res[0]) + '/anime_' + str(Res[1]) +'.tsv', mode = 'r')\n",
    "\n",
    "    Information = FileInfo.read().split('\\n')[1].split('\\t')\n",
    "    Title = Information[0]\n",
    "    Description = Information[10]\n",
    "    URL = FileURL.read().split('\\n')[(Res[0]-1)*50 + Res[1]]\n",
    "    Data.append([Title, Description, URL, DocumentsSimilarity[Res][-1]])\n",
    "    \n",
    "# Here we will sort the documents based on the similarity using heap tree\n",
    "Data = [Doc.docInfo for Doc in heapsort([Docs(i) for i in Data])]\n",
    "\n",
    "# Setting the similarity to 2 float places \n",
    "for doc in Data:\n",
    "    doc[-1] = \"{:.2f}\".format(doc[-1])\n",
    "\n",
    "# Here we will pick the first K documents that are more similar with respect to the given query\n",
    "DataToDataFrame(['animeTitle','animeDescriptio', 'Url', 'Similarity'], Data[:K])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f6d6d",
   "metadata": {},
   "source": [
    "# 3. Define a new score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189ff36",
   "metadata": {},
   "source": [
    "In this question we let the user to include more information in his query. Most of the provided information will be working as a filter except the which user gives us for the synopsis. What we let the user include in his query and in which format: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37df979",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28040d09",
   "metadata": {},
   "source": [
    "1- **The type of anime:** If a user wants to include the type of the anime in the query, he should use this notation -> 1- {The type of anime}, ex. 1- tv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b526405",
   "metadata": {},
   "source": [
    "2- **The date of releasing:** If a user want to include a starting point which our search engine consider the anime which their release that is after that starting poin, he should use this notation -> 2- {released year}, ex. 2- 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7436cd",
   "metadata": {},
   "source": [
    "3- **The date of ending:** If the user wants to include an end point in which our search engine not consider the anime that have been ended after this time then he should use this notation: 3- {end year}, ex. 3- 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6364fc",
   "metadata": {},
   "source": [
    "4- **Some words in the synopsis:** If the user wants to include some words that those words has been occured in the resulted anime's synopsis, he should use this notation: 4- {words in synopsis}, ex. 4- saiyan race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c37d25",
   "metadata": {},
   "source": [
    "* **Note:** We won't consider the documents that are missing some of the information provided in the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59395f2e",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6823ba",
   "metadata": {},
   "source": [
    "To score the related documents and show them in order, we will consider these variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223f047",
   "metadata": {},
   "source": [
    "**The normalized value for value x in a sequence of values X will be defined as (x - min(X))/(max(X) - min(X))**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794b185",
   "metadata": {},
   "source": [
    "1- **The cosine similarity:** The docuemnts that have a higher cosine similarity with respect to the words in synopsis given in the query, will have a higher priority.**To mention this value is between 1 and 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120299b7",
   "metadata": {},
   "source": [
    "2- **Anime score and anime user:** We will consider a combination of anime score and its number of user. We will use a multiplication of the number of the users and the score of the anime. Then when we had this value for all the related animes, we will normalize the values according to its formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a13472",
   "metadata": {},
   "source": [
    "3- **Number of members:** As we know here the higher the member, the more popular the anime. So here we will normalize these values using the mentioned formula for normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b061061",
   "metadata": {},
   "source": [
    "**Final score: The final score of a document in our approach will be the sum of all of these values. The the ones that having higher score, will have a higher priority.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe518e99",
   "metadata": {},
   "source": [
    "* **Note:** If in any case we had some missing values, we will consider those values as 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906806e",
   "metadata": {},
   "source": [
    "### *The main function for this part is at the end of this section*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20845d",
   "metadata": {},
   "source": [
    "Now we go throguh the steps and the functions needed to build this search engine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b58b9",
   "metadata": {},
   "source": [
    "## 1. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15216af9",
   "metadata": {},
   "source": [
    "**Here we first filter the the documents with respect to the given query**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022691f0",
   "metadata": {},
   "source": [
    "### 3.1.1 Function Query Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "038739af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the help of this function we will be able to extract each parameter in the query\n",
    "def QueryProcess(Query):\n",
    "    Strings = re.split('[1|2|3|4]-', Query.lower())[1:] \n",
    "    Numbers = re.findall('[1|2|3|4]+-', Query.lower())\n",
    "    GivenConditions = [\"\".join(i) for i in zip(Numbers, Strings)]\n",
    "    return GivenConditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8badf",
   "metadata": {},
   "source": [
    "After getting the query, now we should filter the documents with respect to the given conditions. Here we will filter the by using 'Anime Type', 'Anime Released Data' and 'Anime End Data'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b3389",
   "metadata": {},
   "source": [
    "### 3.1.2 Function Filter Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d6c5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterDocs(Conditions):\n",
    "    \n",
    "    # This dictionary will contain the filtered documents and their necessary information\n",
    "    FilteredDocs = {}\n",
    "  \n",
    "    # To have the URL of all the anime\n",
    "    FileURL = open('URLs.txt', mode = 'r').read().split('\\n')\n",
    "    \n",
    "    # For each page\n",
    "    for page in range(1, 384):\n",
    "        \n",
    "        # For each anime\n",
    "        for anime in range((50 if page != 383 else 28)):\n",
    "            \n",
    "            # Read the '.tsv' file of each specific anime\n",
    "            Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "            File = open(Path + str(page) + '/anime_' + str(anime) +'.tsv', mode = 'r')\n",
    "            \n",
    "            # Take the URL of a specific anime\n",
    "            URL = FileURL[(page-1)*50 + anime]\n",
    "            \n",
    "            # The extracted information for that anime\n",
    "            AnimeInfo = FilterInfo(File)\n",
    "            \n",
    "            # A flag which specifies if we should consider this docuemnt or it should be filtered\n",
    "            Pick = True\n",
    "            \n",
    "            # For each condition that we have in the query\n",
    "            for Con in Conditions:\n",
    "                \n",
    "                # Here will check each condition given in the query\n",
    "                # Anime type\n",
    "                # Releasing date\n",
    "                # End date\n",
    "                \n",
    "                Con = list(map(str.strip, Con.split('-')))\n",
    "                if Con[0] == '1' and AnimeInfo['animeType'] != Con[1]:\n",
    "                    Pick = False\n",
    "                elif (Con[0] == '2' and AnimeInfo['releaseDate'] < Con[1]) or not AnimeInfo['releaseDate'].isdigit():\n",
    "                    Pick = False\n",
    "                elif (Con[0] == '3' and (AnimeInfo['endDate'] > Con[1]) or not AnimeInfo['endDate'].isdigit()):\n",
    "                    Pick = False \n",
    "            if Pick:\n",
    "                AnimeInfo['URL'] = URL\n",
    "                FilteredDocs[(page,anime)] = deepcopy(AnimeInfo)\n",
    "    return FilteredDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f000f3",
   "metadata": {},
   "source": [
    "### 3.1.3 Function Filter Necessary Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cda23f",
   "metadata": {},
   "source": [
    "In this function given a document we will extract the necessary information which will be 'Anime Title', 'Anime Score', 'Anime Users' and 'Anime members'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2383fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterInfo(Document):\n",
    "    \n",
    "    # We will filter some useful information the given document\n",
    "    ReadDocument = Document.read().split('\\n')[1].split('\\t')\n",
    "    InfoDict = {'animeTitle':ReadDocument[0], 'animeScore': ReadDocument[6], 'animeUser': ReadDocument[7],\n",
    "                'animeMembers': ReadDocument[5], 'releaseDate': ReadDocument[3].split(',')[-1].strip(), \n",
    "                'endDate':ReadDocument[4].split(',')[-1].strip(),'animeType': ReadDocument[1]}\n",
    "    \n",
    "    # We will lower case everything except Anime Title\n",
    "    for i in list(InfoDict.keys())[1:]:\n",
    "        InfoDict[i] = InfoDict[i].lower()\n",
    "    \n",
    "    return deepcopy(InfoDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ac5ce",
   "metadata": {},
   "source": [
    "## 3.1.4 Main Function Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f793c",
   "metadata": {},
   "source": [
    "Here we will get the query from the user in the format stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02e51769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filtering(Query):\n",
    "    FilteredDocuments = FilterDocs(Query)\n",
    "\n",
    "    # Then we should compute check the similarity between the query and the given filtered documents \n",
    "    # So we will add another property to each filtered data name synopsisSimilarity.\n",
    "    # This value will be by default 0. \n",
    "\n",
    "    for Doc in FilteredDocuments:\n",
    "        FilteredDocuments[Doc]['synopsisSimilarity'] = 0 \n",
    "\n",
    "    # Then we will check the cosine similarity between the query and the synosis of the anime if this information\n",
    "    # was provided in the query \n",
    "    \n",
    "    SynopsisQuery = ''\n",
    "    \n",
    "    for q in Query:\n",
    "        if '4-' in q:\n",
    "            SynopsisQuery = q.split('- ')[-1]\n",
    "\n",
    "    if SynopsisQuery:\n",
    "        # Loading the second inverted index term -> documents, # Occurence\n",
    "        SecondInvertedIndex = json.load(open(\"InvertedIndex_2.json\", \"r\")) \n",
    "        \n",
    "        # Loading the third inverted index term -> documents, # tfidf\n",
    "        ThirdInvertedIndex = json.load(open(\"InvertedIndex_3.json\", \"r\"))\n",
    "                  \n",
    "        # Loading the vocabulary file which maps each term to a specific ID\n",
    "        Vocabularies = json.load(open(\"Vocabulary.json\", \"r\"))\n",
    "\n",
    "        # Compute the TFIDF of the query\n",
    "        QueryTFIDF = QueryToTfidf(SynopsisQuery, SecondInvertedIndex,Vocabularies, len(AllURLs))\n",
    "\n",
    "        # Take the documents that are containing all the words\n",
    "        RelatedDocuments = QueryToDocuemnts(QueryTFIDF.keys(), ThirdInvertedIndex)\n",
    "\n",
    "        # Compute the similarity of each document with query\n",
    "        DocumentsSimilarity = Similarity(list(QueryTFIDF.values()), deepcopy(RelatedDocuments))\n",
    "        # Here we will pick the similarity between our filtered data and given query\n",
    "        for doc in FilteredDocuments:\n",
    "            SimilarityOfDoc = DocumentsSimilarity.get(doc)\n",
    "            if SimilarityOfDoc:\n",
    "                FilteredDocuments[doc]['synopsisSimilarity'] = SimilarityOfDoc[-1]\n",
    "    return FilteredDocuments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe44ce",
   "metadata": {},
   "source": [
    "After having all the filtered documents and checking their similarity with respect to the given query, we will now add new scores to each documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557b142",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35a015",
   "metadata": {},
   "source": [
    "In the scoring part as we mentioned above, we will add new scores to each of the filtered documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff4fc1",
   "metadata": {},
   "source": [
    "### 3.2.1 Function Score and User"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48af50c",
   "metadata": {},
   "source": [
    "This function, given all the filtered documents will consider a new score name 'animeSUScore' which is the multiplication of each 'animeScore' and 'animeUser'. At the end we will normalize the values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "872e461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankUser(Docs):\n",
    "    \n",
    "    # Pick the default values for max and min \n",
    "    Min, Max = float(\"inf\"), 0\n",
    "    \n",
    "    # For each document\n",
    "    for doc in Docs: \n",
    "        \n",
    "        # Try to multiply animeScore and animeUser\n",
    "        try:\n",
    "            Docs[doc]['animeSUScore'] = float(Docs[doc]['animeScore']) * float(Docs[doc]['animeUser'])\n",
    "        \n",
    "        # In case of an error we find out the values are missed or corrupted\n",
    "        except: \n",
    "            \n",
    "            # Consider the result of the multiplication 0\n",
    "            Docs[doc]['animeSUScore'] = 0 \n",
    "            \n",
    "        # Update the min and max values\n",
    "        Min, Max = min(Min, Docs[doc]['animeSUScore']), max(Max, Docs[doc]['animeSUScore'])\n",
    "        \n",
    "    # Normalize the values of animeSUScore\n",
    "    for doc in Docs: \n",
    "        Docs[doc]['animeSUScore'] = ( Docs[doc]['animeSUScore'] - Min)/(Max - Min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2befa1",
   "metadata": {},
   "source": [
    "### 3.2.2 Function Member Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40211130",
   "metadata": {},
   "source": [
    "This function given all the filtered documents will normalize the number of members of each document and consider that as a new score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2347834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MemberScore(Docs):\n",
    "    \n",
    "    # Setting the default values for min and max\n",
    "    Min, Max = float('inf'), 0\n",
    "    \n",
    "    # For each document\n",
    "    for doc in Docs:\n",
    "        \n",
    "        # Try to conver the value of animeMembers to float\n",
    "        try:\n",
    "            Docs[doc]['animeMembers'] = float(Docs[doc]['animeMembers'])\n",
    "            \n",
    "        # If corrupted or missed \n",
    "        except: \n",
    "            \n",
    "            # Set it to 0\n",
    "            Docs[doc]['animeMembers'] = 0 \n",
    "        \n",
    "        # Update the min and max\n",
    "        Min, Max = min(Min, Docs[doc]['animeMembers']), max(Max, Docs[doc]['animeMembers'])\n",
    "    \n",
    "    # Normalize the value of memberScore\n",
    "    for doc in Docs:\n",
    "        Docs[doc]['memberScore'] = (Docs[doc]['animeMembers'] - Min)/(Max - Min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ec2a7",
   "metadata": {},
   "source": [
    "### 3.2.3 Function Final Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb880d",
   "metadata": {},
   "source": [
    "This function given all the documents will compute the final score of each document which will be the sum of all the scores we just defined and also the cosine similarity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7daf274",
   "metadata": {},
   "source": [
    "* **None:** As the cosine similarity between the query and the synopsis of the anime gives us more detail about the anime, we will give a coefficient of '3' to this value so that this value dominate the others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96171736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FinalScore(Docs):\n",
    "    # For each document we will compute the final score\n",
    "    for doc in Docs:\n",
    "        Docs[doc]['finalScore'] = Docs[doc]['memberScore'] + Docs[doc]['animeSUScore'] + 3 * Docs[doc]['synopsisSimilarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17792cc",
   "metadata": {},
   "source": [
    "## 3.2.4 Main Score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56df42",
   "metadata": {},
   "source": [
    "This function will go over all the filtered documents and compute the socre of each of the documents and show the documents in order of their final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6594a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will give all the filtered documents to be scored\n",
    "def Scoring(FilteredDocuments):\n",
    "    RankUser(FilteredDocuments)\n",
    "    MemberScore(FilteredDocuments)\n",
    "    FinalScore(FilteredDocuments)\n",
    "    return FilteredDocuments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25159f4b",
   "metadata": {},
   "source": [
    "## 3.3 Main Function Query to Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d367b1",
   "metadata": {},
   "source": [
    "Here we take a query from the user and give back him the resulted documents with respect to the given query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36387f",
   "metadata": {},
   "source": [
    "**Important: Don't forget to use the format of the query when you want to issue a query\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f20d0b",
   "metadata": {},
   "source": [
    "1- {Anime Type} 2- {Anime released date} 3- {Anime end date} 4- {Words in synopsis}<br/><br/>\n",
    "**FYI:** You can issue a subset of the parameters in the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59c6ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query here: 1- tv 4- saiyan race\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Final score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Kai (Dragon Ball Z Kai)</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Death Note</td>\n",
       "      <td>A shinigami, as a god of death, can kill any p...</td>\n",
       "      <td>https://myanimelist.net/anime/1535/Death_Note</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shingeki no Kyojin (Attack on Titan)</td>\n",
       "      <td>Centuries ago, mankind was slaughtered to near...</td>\n",
       "      <td>https://myanimelist.net/anime/16498/Shingeki_n...</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/5114/Fullmetal_A...</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One Punch Man</td>\n",
       "      <td>The seemingly ordinary and unimpressive Saitam...</td>\n",
       "      <td>https://myanimelist.net/anime/30276/One_Punch_Man</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sword Art Online</td>\n",
       "      <td>In the year 2022, virtual reality has progress...</td>\n",
       "      <td>https://myanimelist.net/anime/11757/Sword_Art_...</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Boku no Hero Academia (My Hero Academia)</td>\n",
       "      <td>The appearance of \"quirks,\" newly discovered s...</td>\n",
       "      <td>https://myanimelist.net/anime/31964/Boku_no_He...</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naruto</td>\n",
       "      <td>Moments prior to Naruto Uzumaki's birth, a hug...</td>\n",
       "      <td>https://myanimelist.net/anime/20/Naruto</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tokyo Ghoul</td>\n",
       "      <td>Tokyo has become a cruel and merciless city—a ...</td>\n",
       "      <td>https://myanimelist.net/anime/22319/Tokyo_Ghoul</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 animeTitle  \\\n",
       "0                             Dragon Ball Z   \n",
       "1       Dragon Ball Kai (Dragon Ball Z Kai)   \n",
       "2                                Death Note   \n",
       "3      Shingeki no Kyojin (Attack on Titan)   \n",
       "4          Fullmetal Alchemist: Brotherhood   \n",
       "5                             One Punch Man   \n",
       "6                          Sword Art Online   \n",
       "7  Boku no Hero Academia (My Hero Academia)   \n",
       "8                                    Naruto   \n",
       "9                               Tokyo Ghoul   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "0  Five years after winning the World Martial Art...   \n",
       "1  Five years after the events of Dragon Ball, ma...   \n",
       "2  A shinigami, as a god of death, can kill any p...   \n",
       "3  Centuries ago, mankind was slaughtered to near...   \n",
       "4  After a horrific alchemy experiment goes wrong...   \n",
       "5  The seemingly ordinary and unimpressive Saitam...   \n",
       "6  In the year 2022, virtual reality has progress...   \n",
       "7  The appearance of \"quirks,\" newly discovered s...   \n",
       "8  Moments prior to Naruto Uzumaki's birth, a hug...   \n",
       "9  Tokyo has become a cruel and merciless city—a ...   \n",
       "\n",
       "                                                 Url Final score  \n",
       "0    https://myanimelist.net/anime/813/Dragon_Ball_Z        3.63  \n",
       "1  https://myanimelist.net/anime/6033/Dragon_Ball...        3.18  \n",
       "2      https://myanimelist.net/anime/1535/Death_Note        2.00  \n",
       "3  https://myanimelist.net/anime/16498/Shingeki_n...        1.99  \n",
       "4  https://myanimelist.net/anime/5114/Fullmetal_A...        1.64  \n",
       "5  https://myanimelist.net/anime/30276/One_Punch_Man        1.62  \n",
       "6  https://myanimelist.net/anime/11757/Sword_Art_...        1.51  \n",
       "7  https://myanimelist.net/anime/31964/Boku_no_He...        1.46  \n",
       "8            https://myanimelist.net/anime/20/Naruto        1.37  \n",
       "9    https://myanimelist.net/anime/22319/Tokyo_Ghoul        1.36  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Processing the query and filter the related documents \n",
    "Query = QueryProcess(input(\"Please enter your query here: \"))\n",
    "\n",
    "# Filtering documents based on the given parameters in the query\n",
    "FilteredDocuments = Filtering(Query)\n",
    "\n",
    "# Scoring the filtered documents\n",
    "ScoredDocuments = Scoring(FilteredDocuments)\n",
    "\n",
    "# Here we will store the details of each related documents \n",
    "Data = []\n",
    "for Res in ScoredDocuments:\n",
    "    Path = '/home/mehrdad/ADM-HW3/HTMLS/page'\n",
    "    FileInfo = open(Path + str(Res[0]) + '/anime_' + str(Res[1]) +'.tsv', mode = 'r')\n",
    "    Description = FileInfo.read().split('\\n')[1].split('\\t')[10]\n",
    "\n",
    "    Data.append([ScoredDocuments[Res]['animeTitle'], Description, ScoredDocuments[Res]['URL'],\n",
    "                     ScoredDocuments[Res]['finalScore']])\n",
    "\n",
    "Data = [Doc.docInfo for Doc in heapsort([Docs(i) for i in Data])]\n",
    "\n",
    "# Setting the similarity to 2 float places \n",
    "for doc in Data:\n",
    "    doc[-1] = \"{:.2f}\".format(doc[-1])\n",
    "    \n",
    "# Here we will pick the first K documents that are more similar with respect to the given query\n",
    "DataToDataFrame(['animeTitle','animeDescription', 'Url', 'Final score'], Data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61127ca6",
   "metadata": {},
   "source": [
    "## 3.4 Comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a623d",
   "metadata": {},
   "source": [
    "In this section we will compare the results that we got from this search engine the previous search engine that we have been built in the section 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813609d",
   "metadata": {},
   "source": [
    "The search engine in part 3.3 is more powerful than the search engine in 2.2 due to these: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf557adc",
   "metadata": {},
   "source": [
    "- Search engine in 3.3 gives the user the oppurtunity to use a kind of filtering based on the type, released data and the end date of the anime, while we didn't have this feature in search engine 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dcabea",
   "metadata": {},
   "source": [
    "- Search engine in 3.3 for the resulted anime that have the same cosine similarity will with the help of the other information of the anime can sort the documents with respect to their popularity and rank. The ones that are better will be higher in the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4fb103",
   "metadata": {},
   "source": [
    "- Search engine in 3.3 even if can not find any document that contains all the word given for the synopsis will give us the result based on the other parameters of filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc2f97",
   "metadata": {},
   "source": [
    "- If for a query in synopsis we have less than K related docuements, the search engine in 3.3 can give the result based on the other parameters of the filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d33835",
   "metadata": {},
   "source": [
    "**In the end we discovered that our search engine in 3.3 works much better than the search engine in 2.2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7739d5",
   "metadata": {},
   "source": [
    "# 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcdca6a",
   "metadata": {},
   "source": [
    "## 5.1 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab2b99",
   "metadata": {},
   "source": [
    "Basically in this case we are trying to find the maximum of the elements of an array while these elements are not adjacent. Here we will explain how does our algorithm works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413ca44",
   "metadata": {},
   "source": [
    "We can solve this quetion with a greedy algorithm which solves this question with time complexity of O(n) as we can compute this value by iterating over the values just once. Here you can find the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a41ab",
   "metadata": {},
   "source": [
    "At each step, we will consider two sums. First the maximum sum up to this iteration with the previous element included and another sum which is the maximum sum up to this iteration without the previous element. We call these two sums as 1-SumWithPrevious, 2- SumWithoutPreviuos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d609d",
   "metadata": {},
   "source": [
    "1- We go through the elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761120e",
   "metadata": {},
   "source": [
    "2- As we assume that the previous element has been picked so we add the current element to the SumWithoutPrevious sum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c50ad",
   "metadata": {},
   "source": [
    "3- We check if this value (SumWithoutPrevious) is greater than the value of SumWithPrevious or not. If this was the case then we will swap these two values as for the next item we have picked its adjacent item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c093c91",
   "metadata": {},
   "source": [
    "4- If at some point the value of SumWithoutPrevious + the current element was not greater than SumWithPrevious, it means that with or without the current element we had the maximum sum so far. As we didn't include the element in the sums so we won't have a problem for the next item to be considered. Here we can assign the SumWithoutPrevios to SumWithPrevious. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af825ad5",
   "metadata": {},
   "source": [
    "5- At the end we will return the maximum of these two values, which will be store in SumWithPrevious. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d399838",
   "metadata": {},
   "source": [
    "**You can check a detailed example in 5.3 section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae1716",
   "metadata": {},
   "source": [
    "## 5.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf3a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaximumSumWithoutAdjacent(List):\n",
    "    # define two sums and necessary datastructures\n",
    "    SumWithoutPrevious, SumWithPrevious = 0, 0\n",
    "    \n",
    "    # Showing the elements in the list\n",
    "    print('The elements in the list: ', List)\n",
    "    \n",
    "    # Iterating over the elements\n",
    "    for i in List:\n",
    "        \n",
    "        # Add the current value to the sum without previous\n",
    "        SumWithoutPrevious += i\n",
    "        \n",
    "        # Check the condition \n",
    "        if SumWithoutPrevious > SumWithPrevious:\n",
    "            \n",
    "            # As we gain more score by picking the current element so we swap the values. \n",
    "            SumWithoutPrevious, SumWithPrevious = SumWithPrevious, SumWithoutPrevious\n",
    "        \n",
    "        # If by considering the current element, we don't gain more score \n",
    "        else:\n",
    "\n",
    "            # Up to this point we have the maximum score by not picking the previous element\n",
    "            SumWithoutPrevious = SumWithPrevious\n",
    "            \n",
    "    # Showing the last result. \n",
    "    print('\\nResult:')\n",
    "    print('\\nMaximum sum without considering adjacent elements: \\'', SumWithPrevious, '\\'', sep = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3309cac",
   "metadata": {},
   "source": [
    "### Give me an example here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "926de23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please give me the elements of your list: 30 40 25 50 30 20\n",
      "The elements in the list:  [30, 40, 25, 50, 30, 20]\n",
      "\n",
      "Result:\n",
      "\n",
      "Maximum sum without considering adjacent elements: '110'\n"
     ]
    }
   ],
   "source": [
    "MyList = list(map(int, input(\"Please give me the elements of your list: \").split()))\n",
    "MaximumSumWithoutAdjacent(MyList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005d41f",
   "metadata": {},
   "source": [
    "## 5.3 Steps of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "25f7684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaximumSumWithoutAdjacentSteps(List):\n",
    "    # define two sums and necessary datastructures\n",
    "    SumWithoutPrevious, SumWithPrevious, NotPickLast = 0, 0, False\n",
    "    \n",
    "    # To keep track the picked elements in each sum \n",
    "    WithPrevious, WithoutPrevious = [], []\n",
    "    \n",
    "    # Showing the elements in the list\n",
    "    print('The elements in the list: ', List)\n",
    "    \n",
    "    # Iterating over the elements\n",
    "    for Index, i in enumerate(List):\n",
    "        \n",
    "        # To distinguish between the steps\n",
    "        print('-' * 50)\n",
    "        print('* Step '+str(Index+1)+' *\\n')\n",
    "        \n",
    "        # Explaining of the procedure\n",
    "        print('We are considering number \\'', i, '\\':', sep = '')\n",
    "        print('\\nCurrent value of SumWithPrevious:', SumWithPrevious)\n",
    "        print('Current value of SumWithoutPrevious:', SumWithoutPrevious, end  = '\\n\\n')\n",
    "        \n",
    "        # Add the current value to the sum without previous\n",
    "        SumWithoutPrevious += i\n",
    "        print('Adding value \\'', i, '\\' to SumWithoutPrevious -> SumWithoutPrevious = ', SumWithoutPrevious, sep = '', end = '\\n\\n')\n",
    "        \n",
    "        # Check the condition \n",
    "        if SumWithoutPrevious > SumWithPrevious:\n",
    "            \n",
    "            # As we gain more score by picking the current element so we swap the values. \n",
    "            SumWithoutPrevious, SumWithPrevious = SumWithPrevious, SumWithoutPrevious\n",
    "            \n",
    "            # If we didn't pick the previous element\n",
    "            if NotPickLast:\n",
    "                print('We didn\\'t pick the previous element!!!')\n",
    "                \n",
    "                # As we didn't pick the previous element, we add current value to with previous elements \n",
    "                WithPrevious.append(i)\n",
    "                \n",
    "            # If we have considered the previous element\n",
    "            else:\n",
    "                \n",
    "                # If this is not the first iteration \n",
    "                if Index:\n",
    "                    print('Swap two lists!!!')\n",
    "                \n",
    "                # Now we add current element to sumwithout previous set and swap two lists\n",
    "                WithoutPrevious.append(i)\n",
    "                WithPrevious, WithoutPrevious = WithoutPrevious[:], WithPrevious[:]\n",
    "            \n",
    "            # Showing the values at the current iteration \n",
    "            print('--->> Pick the current element!!!')\n",
    "            print('\\nSumWithoutPrevious is greater than SumWithPrevious -> Swap two maximum!')\n",
    "            print('SumWithPrevious = ', SumWithPrevious, ' SumWithoutPrevious = ', SumWithoutPrevious)\n",
    "            \n",
    "            # We removed the previous element -> we didn't pick the previous element \n",
    "            NotPickLast = False\n",
    "            \n",
    "        # If by considering the current element, we don't gain more score \n",
    "        else:\n",
    "\n",
    "            # Up to this point we have the maximum score by not picking the previous element\n",
    "            SumWithoutPrevious = SumWithPrevious\n",
    "            \n",
    "            WithoutPrevious = WithPrevious[:]\n",
    "            \n",
    "            # We are not picking the current element, so for next iteration we didn't pick the previous one.\n",
    "            NotPickLast = True\n",
    "            \n",
    "            # Showing the details of what we did exactly\n",
    "            print('\\n--->>Don\\'t pick the element!!!')\n",
    "            print('\\nSumWithoutPrevious is not greater than SumWithPrevious -> Both take maximum!')\n",
    "            print('SumWithPrevious = ', SumWithPrevious, ' SumWithoutPrevious = ', SumWithoutPrevious)\n",
    "            \n",
    "        # Showing the picked element up to this point. \n",
    "        print('\\nMaximum sum with current element: ', WithPrevious)\n",
    "        print('\\nMaximum sum without current element: ', WithoutPrevious)\n",
    "        \n",
    "    \n",
    "    # Showing the last result. \n",
    "    print('\\n***************')\n",
    "    print('Result:\\n')\n",
    "    print('All elements:', List)\n",
    "    print('\\nMaximum sum without considering adjacent elements: \\'', SumWithPrevious, '\\'', sep = '')\n",
    "    print('By picking these values: ', WithPrevious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216e74d",
   "metadata": {},
   "source": [
    "### Run the detailed function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "17c6a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elements in the list:  [30, 40, 25, 50, 30, 20]\n",
      "--------------------------------------------------\n",
      "* Step 1 *\n",
      "\n",
      "We are considering number '30':\n",
      "\n",
      "Current value of SumWithPrevious: 0\n",
      "Current value of SumWithoutPrevious: 0\n",
      "\n",
      "Adding value '30' to SumWithoutPrevious -> SumWithoutPrevious = 30\n",
      "\n",
      "--->> Pick the current element!!!\n",
      "\n",
      "SumWithoutPrevious is greater than SumWithPrevious -> Swap two maximum!\n",
      "SumWithPrevious =  30  SumWithoutPrevious =  0\n",
      "\n",
      "Maximum sum with current element:  [30]\n",
      "\n",
      "Maximum sum without current element:  []\n",
      "--------------------------------------------------\n",
      "* Step 2 *\n",
      "\n",
      "We are considering number '40':\n",
      "\n",
      "Current value of SumWithPrevious: 30\n",
      "Current value of SumWithoutPrevious: 0\n",
      "\n",
      "Adding value '40' to SumWithoutPrevious -> SumWithoutPrevious = 40\n",
      "\n",
      "Swap two lists!!!\n",
      "--->> Pick the current element!!!\n",
      "\n",
      "SumWithoutPrevious is greater than SumWithPrevious -> Swap two maximum!\n",
      "SumWithPrevious =  40  SumWithoutPrevious =  30\n",
      "\n",
      "Maximum sum with current element:  [40]\n",
      "\n",
      "Maximum sum without current element:  [30]\n",
      "--------------------------------------------------\n",
      "* Step 3 *\n",
      "\n",
      "We are considering number '25':\n",
      "\n",
      "Current value of SumWithPrevious: 40\n",
      "Current value of SumWithoutPrevious: 30\n",
      "\n",
      "Adding value '25' to SumWithoutPrevious -> SumWithoutPrevious = 55\n",
      "\n",
      "Swap two lists!!!\n",
      "--->> Pick the current element!!!\n",
      "\n",
      "SumWithoutPrevious is greater than SumWithPrevious -> Swap two maximum!\n",
      "SumWithPrevious =  55  SumWithoutPrevious =  40\n",
      "\n",
      "Maximum sum with current element:  [30, 25]\n",
      "\n",
      "Maximum sum without current element:  [40]\n",
      "--------------------------------------------------\n",
      "* Step 4 *\n",
      "\n",
      "We are considering number '50':\n",
      "\n",
      "Current value of SumWithPrevious: 55\n",
      "Current value of SumWithoutPrevious: 40\n",
      "\n",
      "Adding value '50' to SumWithoutPrevious -> SumWithoutPrevious = 90\n",
      "\n",
      "Swap two lists!!!\n",
      "--->> Pick the current element!!!\n",
      "\n",
      "SumWithoutPrevious is greater than SumWithPrevious -> Swap two maximum!\n",
      "SumWithPrevious =  90  SumWithoutPrevious =  55\n",
      "\n",
      "Maximum sum with current element:  [40, 50]\n",
      "\n",
      "Maximum sum without current element:  [30, 25]\n",
      "--------------------------------------------------\n",
      "* Step 5 *\n",
      "\n",
      "We are considering number '30':\n",
      "\n",
      "Current value of SumWithPrevious: 90\n",
      "Current value of SumWithoutPrevious: 55\n",
      "\n",
      "Adding value '30' to SumWithoutPrevious -> SumWithoutPrevious = 85\n",
      "\n",
      "\n",
      "--->>Don't pick the element!!!\n",
      "\n",
      "SumWithoutPrevious is not greater than SumWithPrevious -> Both take maximum!\n",
      "SumWithPrevious =  90  SumWithoutPrevious =  90\n",
      "\n",
      "Maximum sum with current element:  [40, 50]\n",
      "\n",
      "Maximum sum without current element:  [40, 50]\n",
      "--------------------------------------------------\n",
      "* Step 6 *\n",
      "\n",
      "We are considering number '20':\n",
      "\n",
      "Current value of SumWithPrevious: 90\n",
      "Current value of SumWithoutPrevious: 90\n",
      "\n",
      "Adding value '20' to SumWithoutPrevious -> SumWithoutPrevious = 110\n",
      "\n",
      "We didn't pick the previous element!!!\n",
      "--->> Pick the current element!!!\n",
      "\n",
      "SumWithoutPrevious is greater than SumWithPrevious -> Swap two maximum!\n",
      "SumWithPrevious =  110  SumWithoutPrevious =  90\n",
      "\n",
      "Maximum sum with current element:  [40, 50, 20]\n",
      "\n",
      "Maximum sum without current element:  [40, 50]\n",
      "\n",
      "***************\n",
      "Result:\n",
      "\n",
      "All elements: [30, 40, 25, 50, 30, 20]\n",
      "\n",
      "Maximum sum without considering adjacent elements: '110'\n",
      "By picking these values:  [40, 50, 20]\n"
     ]
    }
   ],
   "source": [
    "MaximumSumWithoutAdjacentSteps(MyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739e940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
